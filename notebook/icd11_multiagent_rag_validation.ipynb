{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984e5c1a",
   "metadata": {},
   "source": [
    "# ICD-11 Multi-Agent RAG System — Local Validation on Apple Silicon\n",
    "\n",
    "---\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Version** | 1.0.0 |\n",
    "| **Date** | February 19th, 2026 |\n",
    "| **Platform** | Apple Silicon M1/M2 · macOS · 16 GB RAM |\n",
    "| **Python** | ≥ 3.11 |\n",
    "| **License** | Educational use only — Not for clinical deployment |\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook provides an **end-to-end validation** of a clinical decision-support prototype built on **Retrieval-Augmented Generation (RAG)** and a **multi-agent architecture**, running entirely locally on Apple Silicon hardware.\n",
    "\n",
    "The system ingests real **ICD-11 (CIE-11)** PDF documents, builds a hybrid vector/lexical index, and orchestrates three specialised agents: a *Therapist*, a *Client simulator*, and a *Diagnostician*, to produce structured, evidence-grounded diagnostic hypotheses in Spanish.\n",
    "\n",
    "### Key contributions\n",
    "- Fully local inference stack (no cloud API required)\n",
    "- Metal GPU acceleration via PyTorch MPS + llama-cpp-python\n",
    "- Hybrid semantic retrieval: dense embeddings (PubMedBERT) + BM25\n",
    "- Three-agent conversational pipeline with a built-in safety gate\n",
    "- Validated on 16 GB Apple Silicon under memory-optimised settings\n",
    "\n",
    "> **Disclaimer**: This system is intended exclusively for **educational and research** purposes. It must not be used to inform real clinical decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc290db",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [Install Dependencies](#2-install-dependencies)\n",
    "3. [Verify Installations](#3-verify-installations)\n",
    "4. [Metal / MPS Acceleration Benchmark](#4-metal--mps-acceleration-benchmark)\n",
    "5. [Model Downloads](#5-model-downloads)\n",
    "6. [LLM Initialisation](#6-llm-initialisation)\n",
    "7. [Data Ingestion — Load ICD-11 PDF](#7-data-ingestion--load-icd-11-pdf)\n",
    "8. [Text Parsing & Extraction](#8-text-parsing--extraction)\n",
    "9. [Semantic Chunking](#9-semantic-chunking)\n",
    "10. [Vector Store — ChromaDB](#10-vector-store--chromadb)\n",
    "11. [Hybrid Retrieval — Dense + BM25](#11-hybrid-retrieval--dense--bm25)\n",
    "12. [Multi-Agent Architecture](#12-multi-agent-architecture)\n",
    "13. [Session Simulation](#13-session-simulation)\n",
    "14. [RAG-Enhanced Diagnosis](#14-rag-enhanced-diagnosis)\n",
    "15. [Performance Metrics](#15-performance-metrics)\n",
    "16. [Safety Gate Validation](#16-safety-gate-validation)\n",
    "17. [Validation Summary](#17-validation-summary)\n",
    "18. [Cleanup & Session Persistence](#18-cleanup--session-persistence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d8a55",
   "metadata": {},
   "source": [
    "## Quick-Start Guide\n",
    "\n",
    "### Prerequisites\n",
    "```\n",
    "macOS 13+ · Apple Silicon (M1/M2)\n",
    "Python 3.11–3.13  ·  16 GB RAM recommended\n",
    "```\n",
    "\n",
    "### First-time setup (run once in terminal)\n",
    "```bash\n",
    "# 1. Create and activate a virtual environment\n",
    "python -m venv .venv && source .venv/bin/activate\n",
    "\n",
    "# 2. Install all dependencies (see Section 2 for details)\n",
    "pip install torch torchvision torchaudio\n",
    "pip install langchain>=0.3.14 langchain-community>=0.3.14 langchain-chroma>=0.2.2\n",
    "pip install chromadb>=0.5.23 rank-bm25>=0.2.2\n",
    "pip install sentence-transformers>=3.3.0 transformers>=4.47.0\n",
    "pip install pymupdf>=1.25.0 huggingface-hub>=0.27.0 tqdm numpy pyyaml pydantic>=2.10.0\n",
    "\n",
    "# 3. llama-cpp-python with Metal backend (required for GPU acceleration)\n",
    "CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install -U llama-cpp-python --no-cache-dir\n",
    "```\n",
    "\n",
    "### Run modes\n",
    "\n",
    "| `MODE` | `MAX_PDF_PAGES` | `SESSION_TURNS` | Duration |\n",
    "|--------|----------------|-----------------|----------|\n",
    "| `lightweight` | 10 | 2 | ~5 min |\n",
    "| `standard` | 50 | 4 | ~15 min |\n",
    "| `full` | None (all) | 6 | ~30 min |\n",
    "\n",
    "Set these in **Section 1** before running the notebook.\n",
    "\n",
    "### CIE-11 PDF\n",
    "Place the official document at `<project_root>/files/cie11.pdf`.  \n",
    "If not found, the notebook falls back to a built-in sample with the three most relevant codes (6A70 · 6A71 · 6A72).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41493cb",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "\n",
    "Configure global parameters, detect hardware, and initialise the directory structure.\n",
    "\n",
    "> **Tip**: Adjust `CONFIG` here once — every subsequent cell reads from it automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70753b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  ICD-11 MULTI-AGENT RAG SYSTEM — RUNTIME CONFIGURATION\n",
      "======================================================================\n",
      "  MODE                      lightweight\n",
      "  MAX_PDF_PAGES             10\n",
      "  DOWNLOAD_MODELS           False\n",
      "  BATCH_SIZE                4\n",
      "  SESSION_TURNS             2\n",
      "  ENABLE_DIAGNOSTICIAN      True\n",
      "  MEMORY_CLEANUP            True\n",
      "======================================================================\n",
      "\n",
      "  To switch to FULL mode (entire PDF, more turns):\n",
      "  • MODE           = 'full'\n",
      "  • MAX_PDF_PAGES  = None  (or a large integer)\n",
      "  • SESSION_TURNS  = 6\n",
      "  • DOWNLOAD_MODELS = True  (first run only)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ─── Global configuration ───────────────────────────────────────────────────\n",
    "CONFIG = {\n",
    "    # Execution mode: \"lightweight\" | \"standard\" | \"full\"\n",
    "    \"MODE\": \"lightweight\",\n",
    "    # Maximum PDF pages to process (None = all pages)\n",
    "    \"MAX_PDF_PAGES\": 10,\n",
    "    # Set True only on first run to download models from HuggingFace\n",
    "    \"DOWNLOAD_MODELS\": False,\n",
    "    # Embedding generation batch size (reduce if OOM errors appear)\n",
    "    \"BATCH_SIZE\": 4,\n",
    "    # Number of therapist–client conversation turns to simulate\n",
    "    \"SESSION_TURNS\": 2,\n",
    "    # Enable the diagnostician agent (requires LLM)\n",
    "    \"ENABLE_DIAGNOSTICIAN\": True,\n",
    "    # Release GPU/CPU memory between pipeline stages\n",
    "    \"MEMORY_CLEANUP\": True,\n",
    "}\n",
    "\n",
    "# ─── Pretty-print configuration ─────────────────────────────────────────────\n",
    "print(\"=\" * 70)\n",
    "print(\"  ICD-11 MULTI-AGENT RAG SYSTEM — RUNTIME CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key:<25} {value}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"  To switch to FULL mode (entire PDF, more turns):\")\n",
    "print(\"  • MODE           = 'full'\")\n",
    "print(\"  • MAX_PDF_PAGES  = None  (or a large integer)\")\n",
    "print(\"  • SESSION_TURNS  = 6\")\n",
    "print(\"  • DOWNLOAD_MODELS = True  (first run only)\")\n",
    "print()\n",
    "\n",
    "# ─── Memory cleanup helper ──────────────────────────────────────────────────\n",
    "def cleanup_memory(stage: str = \"\") -> None:\n",
    "    \"\"\"Release cached tensors and run Python garbage collection.\"\"\"\n",
    "    if not CONFIG[\"MEMORY_CLEANUP\"]:\n",
    "        return\n",
    "    gc.collect()\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051fc30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root : /rag-project\n",
      "Data dir     : /rag-project/data\n",
      "Persist dir  : /rag-project/data/icd11_rag_data\n",
      "\n",
      "=== Apple Silicon / MPS Detection ===\n",
      "PyTorch version : 2.10.0\n",
      "MPS available   : True\n",
      "MPS built       : True\n",
      "✓ Metal GPU acceleration enabled\n",
      "Active device   : mps\n",
      "\n",
      "=== Directory Structure ===\n",
      "  ✓ /rag-project/data/icd11_rag_data/models\n",
      "  ✓ /rag-project/data/icd11_rag_data/data/raw\n",
      "  ✓ /rag-project/data/icd11_rag_data/data/chunks\n",
      "  ✓ /rag-project/data/icd11_rag_data/data/indexes\n",
      "  ✓ /rag-project/data/files\n",
      "\n",
      "Environment initialised successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# ─── Project paths ──────────────────────────────────────────────────────────\n",
    "# Assumes this notebook lives at  <project_root>/notebook/\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\"\n",
    "PERSIST_DIR  = DATA_DIR / \"icd11_rag_data\"\n",
    "\n",
    "def display_path(path: Path) -> str:\n",
    "    \"\"\"Render paths relative to the project root for cleaner notebook output.\"\"\"\n",
    "    try:\n",
    "        rel = path.relative_to(PROJECT_ROOT)\n",
    "        if str(rel) == \".\":\n",
    "            return f\"/{PROJECT_ROOT.name}\"\n",
    "        return f\"/{PROJECT_ROOT.name}/{rel}\"\n",
    "    except ValueError:\n",
    "        return str(path)\n",
    "\n",
    "print(f\"Project root : {display_path(PROJECT_ROOT)}\")\n",
    "print(f\"Data dir     : {display_path(DATA_DIR)}\")\n",
    "print(f\"Persist dir  : {display_path(PERSIST_DIR)}\")\n",
    "\n",
    "# ─── Hardware detection (Apple Silicon MPS) ──────────────────────────────────\n",
    "print()\n",
    "print(\"=== Apple Silicon / MPS Detection ===\")\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "mps_available = torch.backends.mps.is_available()\n",
    "mps_built     = torch.backends.mps.is_built()\n",
    "print(f\"MPS available   : {mps_available}\")\n",
    "print(f\"MPS built       : {mps_built}\")\n",
    "\n",
    "if mps_available:\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✓ Metal GPU acceleration enabled\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS unavailable — falling back to CPU\")\n",
    "\n",
    "gpu_available = mps_available\n",
    "print(f\"Active device   : {device}\")\n",
    "\n",
    "# ─── Directory structure ─────────────────────────────────────────────────────\n",
    "print()\n",
    "print(\"=== Directory Structure ===\")\n",
    "directories = [\n",
    "    PERSIST_DIR / \"models\",\n",
    "    PERSIST_DIR / \"data\" / \"raw\",\n",
    "    PERSIST_DIR / \"data\" / \"chunks\",\n",
    "    PERSIST_DIR / \"data\" / \"indexes\",\n",
    "    DATA_DIR    / \"files\",\n",
    "]\n",
    "for d in directories:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"  ✓ {display_path(d)}\")\n",
    "print()\n",
    "print(\"Environment initialised successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6acf261",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Install Dependencies\n",
    "\n",
    "All packages are listed by category below.  \n",
    "**Run the terminal commands once, then restart the kernel and continue from Section 3.**\n",
    "\n",
    "| Category | Key packages |\n",
    "|---|---|\n",
    "| Core ML | `torch`, `torchvision`, `torchaudio` |\n",
    "| LLM inference | `llama-cpp-python` (Metal build) |\n",
    "| Orchestration | `langchain`, `langchain-community`, `langchain-chroma` |\n",
    "| Vector store | `chromadb` |\n",
    "| Lexical retrieval | `rank-bm25` |\n",
    "| Embeddings | `sentence-transformers`, `transformers` |\n",
    "| PDF processing | `pymupdf` |\n",
    "| Utilities | `huggingface-hub`, `tqdm`, `numpy`, `pydantic`, `pyyaml` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cabc1ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Python Environment ===\n",
      "Virtual environment : ✓ active\n",
      "Python version      : 3.13.3\n",
      "Executable          : /Users/ketcx/pinguino_project/.venv/bin/python\n",
      "\n",
      "=== Dependency Catalogue ===\n",
      "\n",
      "Core ML:\n",
      "  • torch\n",
      "  • torchvision\n",
      "  • torchaudio\n",
      "\n",
      "LLM Inference (Metal):\n",
      "  • llama-cpp-python\n",
      "\n",
      "LangChain Ecosystem:\n",
      "  • langchain>=0.3.14\n",
      "  • langchain-community>=0.3.14\n",
      "  • langchain-chroma>=0.2.2\n",
      "  • langgraph>=0.2.60\n",
      "\n",
      "Vector Store & Retrieval:\n",
      "  • chromadb>=0.5.23\n",
      "  • rank-bm25>=0.2.2\n",
      "\n",
      "Embeddings & NLP:\n",
      "  • sentence-transformers>=3.3.0\n",
      "  • transformers>=4.47.0\n",
      "\n",
      "PDF & Utilities:\n",
      "  • pymupdf>=1.25.0\n",
      "  • pydantic>=2.10.0\n",
      "  • pyyaml>=6.0.2\n",
      "  • huggingface-hub>=0.27.0\n",
      "  • tqdm\n",
      "  • numpy\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Installation commands (run in terminal, not here):\n",
      "\n",
      "  pip install torch torchvision torchaudio\n",
      "  pip install langchain>=0.3.14 langchain-community>=0.3.14 \\\n",
      "              langchain-chroma>=0.2.2 langgraph>=0.2.60\n",
      "  pip install chromadb>=0.5.23 rank-bm25>=0.2.2\n",
      "  pip install sentence-transformers>=3.3.0 transformers>=4.47.0\n",
      "  pip install pymupdf>=1.25.0 pydantic>=2.10.0 pyyaml>=6.0.2 \\\n",
      "              huggingface-hub>=0.27.0 tqdm numpy\n",
      "\n",
      "  # llama-cpp-python with Metal GPU support (Apple Silicon)\n",
      "  CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install -U llama-cpp-python --no-cache-dir\n"
     ]
    }
   ],
   "source": [
    "# ─── Dependency catalogue (for reference and auditing) ──────────────────────\n",
    "import sys\n",
    "\n",
    "print(\"=== Python Environment ===\")\n",
    "in_venv = hasattr(sys, \"real_prefix\") or (\n",
    "    hasattr(sys, \"base_prefix\") and sys.base_prefix != sys.prefix\n",
    ")\n",
    "print(f\"Virtual environment : {'✓ active' if in_venv else '⚠ not detected'}\")\n",
    "print(f\"Python version      : {sys.version.split()[0]}\")\n",
    "print(f\"Executable          : {sys.executable}\")\n",
    "\n",
    "DEPENDENCIES = {\n",
    "    \"Core ML\": [\n",
    "        \"torch\", \"torchvision\", \"torchaudio\",\n",
    "    ],\n",
    "    \"LLM Inference (Metal)\": [\n",
    "        \"llama-cpp-python\",  # Build with: CMAKE_ARGS=\"-DLLAMA_METAL=on\"\n",
    "    ],\n",
    "    \"LangChain Ecosystem\": [\n",
    "        \"langchain>=0.3.14\",\n",
    "        \"langchain-community>=0.3.14\",\n",
    "        \"langchain-chroma>=0.2.2\",\n",
    "        \"langgraph>=0.2.60\",\n",
    "    ],\n",
    "    \"Vector Store & Retrieval\": [\n",
    "        \"chromadb>=0.5.23\",\n",
    "        \"rank-bm25>=0.2.2\",\n",
    "    ],\n",
    "    \"Embeddings & NLP\": [\n",
    "        \"sentence-transformers>=3.3.0\",\n",
    "        \"transformers>=4.47.0\",\n",
    "    ],\n",
    "    \"PDF & Utilities\": [\n",
    "        \"pymupdf>=1.25.0\",\n",
    "        \"pydantic>=2.10.0\",\n",
    "        \"pyyaml>=6.0.2\",\n",
    "        \"huggingface-hub>=0.27.0\",\n",
    "        \"tqdm\",\n",
    "        \"numpy\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "print()\n",
    "print(\"=== Dependency Catalogue ===\")\n",
    "for category, packages in DEPENDENCIES.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for pkg in packages:\n",
    "        print(f\"  • {pkg}\")\n",
    "\n",
    "print()\n",
    "print(\"─\" * 60)\n",
    "print(\"Installation commands (run in terminal, not here):\")\n",
    "print()\n",
    "print(\"  pip install torch torchvision torchaudio\")\n",
    "print(\"  pip install langchain>=0.3.14 langchain-community>=0.3.14 \\\\\")\n",
    "print(\"              langchain-chroma>=0.2.2 langgraph>=0.2.60\")\n",
    "print(\"  pip install chromadb>=0.5.23 rank-bm25>=0.2.2\")\n",
    "print(\"  pip install sentence-transformers>=3.3.0 transformers>=4.47.0\")\n",
    "print(\"  pip install pymupdf>=1.25.0 pydantic>=2.10.0 pyyaml>=6.0.2 \\\\\")\n",
    "print(\"              huggingface-hub>=0.27.0 tqdm numpy\")\n",
    "print()\n",
    "print(\"  # llama-cpp-python with Metal GPU support (Apple Silicon)\")\n",
    "print(\"  CMAKE_ARGS=\\\"-DLLAMA_METAL=on\\\" pip install -U llama-cpp-python --no-cache-dir\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb0c88b",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Verify Installations\n",
    "\n",
    "Confirm that every dependency can be imported and that MPS is operational.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e115c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  INSTALLATION VERIFICATION\n",
      "======================================================================\n",
      "  ✓ Python                                   3.13.3\n",
      "  ✓ NumPy                                    2.4.1\n",
      "  ✓ PyTorch                                  2.10.0 (MPS=✓)\n",
      "  ✓ llama-cpp-python                         OK\n",
      "  ✓ LangChain ecosystem                      OK\n",
      "  ✓ sentence-transformers                    OK\n",
      "  ✓ ChromaDB · BM25 · HuggingFace Hub        chromadb 1.5.0\n",
      "  ✓ MPS tensor operation                     OK\n",
      "\n",
      "  Result : 8/8 checks passed\n",
      "  ✅ All dependencies verified — ready to continue\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  INSTALLATION VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = {}\n",
    "\n",
    "def check(label, fn):\n",
    "    try:\n",
    "        version = fn()\n",
    "        print(f\"  ✓ {label:<40} {version}\")\n",
    "        results[label] = True\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {label:<40} {e}\")\n",
    "        results[label] = False\n",
    "\n",
    "check(\"Python\",   lambda: sys.version.split()[0])\n",
    "check(\"NumPy\",    lambda: np.__version__)\n",
    "check(\"PyTorch\",  lambda: f\"{torch.__version__} (MPS={'✓' if torch.backends.mps.is_available() else '✗'})\")\n",
    "\n",
    "def check_llama():\n",
    "    from llama_cpp import Llama\n",
    "    return \"OK\"\n",
    "check(\"llama-cpp-python\", check_llama)\n",
    "\n",
    "def check_langchain():\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    return \"OK\"\n",
    "check(\"LangChain ecosystem\", check_langchain)\n",
    "\n",
    "def check_st():\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    return \"OK\"\n",
    "check(\"sentence-transformers\", check_st)\n",
    "\n",
    "def check_infra():\n",
    "    import chromadb\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    return f\"chromadb {chromadb.__version__}\"\n",
    "check(\"ChromaDB · BM25 · HuggingFace Hub\", check_infra)\n",
    "\n",
    "def check_mps():\n",
    "    t = torch.randn(64, 64, device=\"mps\")\n",
    "    _ = t @ t.T\n",
    "    return \"OK\"\n",
    "check(\"MPS tensor operation\", check_mps)\n",
    "\n",
    "print()\n",
    "passed = sum(results.values())\n",
    "total  = len(results)\n",
    "print(f\"  Result : {passed}/{total} checks passed\")\n",
    "if passed == total:\n",
    "    print(\"  ✅ All dependencies verified — ready to continue\")\n",
    "else:\n",
    "    print(\"  ⚠  Review failed checks and reinstall missing packages\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911bbf4",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Metal / MPS Acceleration Benchmark\n",
    "\n",
    "Measure the GPU speedup provided by Apple Metal Performance Shaders over CPU for matrix operations — a representative proxy for embedding and model inference workloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3616389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Metal / MPS Performance Benchmark ===\n",
      "PyTorch : 2.10.0\n",
      "MPS     : available=True, built=True\n",
      "\n",
      "  Matrix size  : 2000 × 2000\n",
      "  Iterations   : 50\n",
      "  MPS time     : 0.6403 s\n",
      "  CPU time     : 0.9495 s\n",
      "  Speedup      : 1.48×\n",
      "\n",
      "  ⚠  Speedup below expected range — verify Metal build\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"=== Metal / MPS Performance Benchmark ===\")\n",
    "print(f\"PyTorch : {torch.__version__}\")\n",
    "print(f\"MPS     : available={torch.backends.mps.is_available()}, built={torch.backends.mps.is_built()}\")\n",
    "print()\n",
    "\n",
    "SIZE       = 2_000   # matrix dimension\n",
    "ITERATIONS = 50      # repetitions for stable timing\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    # Warm-up\n",
    "    _ = torch.randn(SIZE, SIZE, device=\"mps\") @ torch.randn(SIZE, SIZE, device=\"mps\")\n",
    "    torch.mps.synchronize()\n",
    "\n",
    "    # MPS benchmark\n",
    "    x_mps = torch.randn(SIZE, SIZE, device=\"mps\")\n",
    "    y_mps = torch.randn(SIZE, SIZE, device=\"mps\")\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(ITERATIONS):\n",
    "        torch.matmul(x_mps, y_mps)\n",
    "    torch.mps.synchronize()\n",
    "    mps_time = time.perf_counter() - t0\n",
    "\n",
    "    # CPU benchmark\n",
    "    x_cpu = torch.randn(SIZE, SIZE)\n",
    "    y_cpu = torch.randn(SIZE, SIZE)\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(ITERATIONS):\n",
    "        torch.matmul(x_cpu, y_cpu)\n",
    "    cpu_time = time.perf_counter() - t0\n",
    "\n",
    "    speedup = cpu_time / mps_time\n",
    "    print(f\"  Matrix size  : {SIZE} × {SIZE}\")\n",
    "    print(f\"  Iterations   : {ITERATIONS}\")\n",
    "    print(f\"  MPS time     : {mps_time:.4f} s\")\n",
    "    print(f\"  CPU time     : {cpu_time:.4f} s\")\n",
    "    print(f\"  Speedup      : {speedup:.2f}×\")\n",
    "    print()\n",
    "    if speedup >= 2:\n",
    "        print(\"  ✅ Metal acceleration confirmed\")\n",
    "    else:\n",
    "        print(\"  ⚠  Speedup below expected range — verify Metal build\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"  ⚠  MPS unavailable — all computation will run on CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409bfa4",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Downloads\n",
    "\n",
    "Download the two models that power the pipeline:\n",
    "\n",
    "| Model | Source | Size | Purpose |\n",
    "|---|---|---|---|\n",
    "| **Phi-3-mini-4k-instruct Q4_K_M** | `bartowski/Phi-3-mini-4k-instruct-GGUF` | ~2.3 GB | Local LLM inference |\n",
    "| **PubMedBERT-base-embeddings** | `NeuML/pubmedbert-base-embeddings` | ~420 MB | Semantic embeddings |\n",
    "\n",
    "<br />\n",
    "\n",
    "> Set `DOWNLOAD_MODELS = True` in Section 1 to trigger downloads.  \n",
    "> On subsequent runs, cached models are reused automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa7363c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Download & Loading ===\n",
      "\n",
      "[1/2] LLM — Phi-3-mini (GGUF Q4_K_M)\n",
      "  ✓ Using cached model: Phi-3-mini-4k-instruct-Q4_K_M.gguf  (2.39 GB)\n",
      "\n",
      "[2/2] Embeddings — PubMedBERT-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e374c54f6ef42f9a09123891b613ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Loaded  (dim=768, device=mps:0)\n",
      "\n",
      "✅ Model setup complete\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"=== Model Download & Loading ===\")\n",
    "\n",
    "# ─── 1. LLM (GGUF format for llama-cpp-python) ──────────────────────────────\n",
    "print(\"\\n[1/2] LLM — Phi-3-mini (GGUF Q4_K_M)\")\n",
    "llm_path = None\n",
    "\n",
    "if CONFIG[\"DOWNLOAD_MODELS\"]:\n",
    "    candidates = [\n",
    "        (\"bartowski/Phi-3-mini-4k-instruct-GGUF\", \"Phi-3-mini-4k-instruct-Q4_K_M.gguf\"),\n",
    "        (\"bartowski/Mistral-7B-Instruct-v0.3-GGUF\", \"Mistral-7B-Instruct-v0.3.Q4_K_M.gguf\"),\n",
    "    ]\n",
    "    for repo_id, filename in candidates:\n",
    "        try:\n",
    "            print(f\"  Downloading from {repo_id} …\")\n",
    "            llm_path = hf_hub_download(\n",
    "                repo_id=repo_id,\n",
    "                filename=filename,\n",
    "                cache_dir=str(PERSIST_DIR / \"models\"),\n",
    "            )\n",
    "            size_gb = os.path.getsize(llm_path) / 1e9\n",
    "            print(f\"  ✓ {filename}  ({size_gb:.2f} GB)\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {repo_id}: {str(e)[:80]}\")\n",
    "else:\n",
    "    # Attempt to reuse a previously cached model\n",
    "    for f in (PERSIST_DIR / \"models\").rglob(\"*.gguf\"):\n",
    "        llm_path = str(f)\n",
    "        print(f\"  ✓ Using cached model: {f.name}  ({f.stat().st_size / 1e9:.2f} GB)\")\n",
    "        break\n",
    "    if not llm_path:\n",
    "        print(\"  ⚠  No cached model found — set DOWNLOAD_MODELS=True to download\")\n",
    "\n",
    "# ─── 2. Embedding model ──────────────────────────────────────────────────────\n",
    "print(\"\\n[2/2] Embeddings — PubMedBERT-base\")\n",
    "embeddings_model = None\n",
    "try:\n",
    "    embeddings_model = SentenceTransformer(\n",
    "        \"NeuML/pubmedbert-base-embeddings\",\n",
    "        cache_folder=str(PERSIST_DIR / \"models\"),\n",
    "        device=\"mps\" if gpu_available else \"cpu\",\n",
    "    )\n",
    "    dim = embeddings_model.get_sentence_embedding_dimension()\n",
    "    print(f\"  ✓ Loaded  (dim={dim}, device={embeddings_model.device})\")\n",
    "    cleanup_memory(\"embeddings_load\")\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Failed to load embeddings: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"✅ Model setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc5ad19",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. LLM Initialisation\n",
    "\n",
    "Load the GGUF model into `llama-cpp-python` with Metal acceleration.\n",
    "\n",
    "**Memory-optimised settings for 16 GB Apple Silicon:**\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|---|---|---|\n",
    "| `n_ctx` | 2 048 | Fits comfortably in 16 GB without swapping |\n",
    "| `n_gpu_layers` | 1 | Activates Metal offload |\n",
    "| `n_threads` | 4 | Avoids P/E-core contention on M1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478c32f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Initialisation ===\n",
      "  Model      : Phi-3-mini-4k-instruct-Q4_K_M.gguf\n",
      "  Chat format: chatml\n",
      "\n",
      "  Attempting Metal initialisation …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ LLM ready (Metal)\n",
      "  ✓ Test inference: 'Yes'\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "print(\"=== LLM Initialisation ===\")\n",
    "llm = None\n",
    "\n",
    "if llm_path:\n",
    "    # Select chat format based on model family\n",
    "    model_name  = llm_path.lower()\n",
    "    chat_format = \"chatml\" if \"phi\" in model_name else (\n",
    "                  \"mistral-instruct\" if \"mistral\" in model_name else None)\n",
    "\n",
    "    print(f\"  Model      : {Path(llm_path).name}\")\n",
    "    print(f\"  Chat format: {chat_format or 'auto-detect'}\")\n",
    "    print()\n",
    "\n",
    "    for gpu_layers, label in [(1, \"Metal\"), (0, \"CPU\")]:\n",
    "        try:\n",
    "            print(f\"  Attempting {label} initialisation …\")\n",
    "            llm = Llama(\n",
    "                model_path=llm_path,\n",
    "                n_ctx=2048,\n",
    "                n_gpu_layers=gpu_layers,\n",
    "                n_threads=4,\n",
    "                verbose=False,\n",
    "                chat_format=chat_format,\n",
    "            )\n",
    "            print(f\"  ✓ LLM ready ({label})\")\n",
    "\n",
    "            # Sanity-check inference\n",
    "            resp = llm.create_chat_completion(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\",   \"content\": \"Reply with one word: ready\"},\n",
    "                ],\n",
    "                max_tokens=8,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            print(f\"  ✓ Test inference: '{resp['choices'][0]['message']['content'].strip()}'\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {label} failed: {str(e)[:100]}\")\n",
    "            llm = None\n",
    "\n",
    "    if not llm:\n",
    "        print(\"  ✗ LLM could not be initialised — agent steps will be skipped\")\n",
    "else:\n",
    "    print(\"  ⚠  No model path available — set DOWNLOAD_MODELS=True and re-run Section 5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b8a34",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Data Ingestion — Load ICD-11 PDF\n",
    "\n",
    "Locate the CIE-11 source document.  \n",
    "If the PDF is present at `<project_root>/files/cie11.pdf`, it is used directly.  \n",
    "Otherwise a built-in Spanish sample covering codes **6A70** · **6A71** · **6A72** is generated as a fallback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05367081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Document Loading ===\n",
      "  ✓ CIE-11 PDF found  (5.6 MB, 404 pages)\n",
      "\n",
      "  Source     : Real CIE-11 PDF\n",
      "  Path       : /rag-project/files/cie11.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "pdf_file    = PROJECT_ROOT / \"files\" / \"cie11.pdf\"\n",
    "sample_file = DATA_DIR / \"files\" / \"sample_cie11.txt\"\n",
    "\n",
    "print(\"=== Document Loading ===\")\n",
    "\n",
    "use_pdf = False\n",
    "if pdf_file.exists():\n",
    "    try:\n",
    "        doc = fitz.open(str(pdf_file))\n",
    "        n_pages = len(doc)\n",
    "        doc.close()\n",
    "        print(f\"  ✓ CIE-11 PDF found  ({pdf_file.stat().st_size / 1e6:.1f} MB, {n_pages} pages)\")\n",
    "        source_file = pdf_file\n",
    "        use_pdf = True\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error opening PDF: {e} — falling back to sample text\")\n",
    "else:\n",
    "    print(f\"  ⚠  PDF not found at: {display_path(pdf_file)}\")\n",
    "    print(\"      Generating built-in sample …\")\n",
    "\n",
    "if not use_pdf:\n",
    "    SAMPLE_TEXT = \"\"\"\n",
    "6A70 Trastorno depresivo\n",
    "\n",
    "El trastorno depresivo se caracteriza por un estado de ánimo deprimido o una pérdida\n",
    "de placer o interés en actividades durante la mayor parte del día, casi todos los días,\n",
    "durante un período de al menos dos semanas.\n",
    "\n",
    "Criterios diagnósticos:\n",
    "- Estado de ánimo deprimido\n",
    "- Pérdida de interés o placer\n",
    "- Cambios en el apetito o peso\n",
    "- Alteraciones del sueño\n",
    "- Fatiga o pérdida de energía\n",
    "- Sentimientos de inutilidad o culpa excesiva\n",
    "- Dificultad para concentrarse\n",
    "- Pensamientos de muerte o suicidio\n",
    "\n",
    "Duración mínima: 2 semanas.\n",
    "Impacto funcional: deterioro significativo en áreas importantes de funcionamiento.\n",
    "\n",
    "6A71 Trastorno de ansiedad generalizada\n",
    "\n",
    "Se caracteriza por ansiedad y preocupación excesivas y persistentes que ocurren la\n",
    "mayoría de los días durante al menos varios meses.\n",
    "\n",
    "Criterios diagnósticos:\n",
    "- Preocupación excesiva difícil de controlar\n",
    "- Inquietud o sensación de estar al límite\n",
    "- Fatiga fácil\n",
    "- Dificultad para concentrarse\n",
    "- Irritabilidad\n",
    "- Tensión muscular\n",
    "- Alteraciones del sueño\n",
    "\n",
    "Los síntomas producen deterioro significativo en el funcionamiento.\n",
    "\n",
    "6A72 Trastorno de pánico\n",
    "\n",
    "Ataques de pánico recurrentes e inesperados. Episodio discreto de miedo o aprensión\n",
    "intensos con inicio súbito.\n",
    "\n",
    "Síntomas físicos y cognitivos típicos:\n",
    "- Palpitaciones o ritmo cardíaco acelerado\n",
    "- Sudoración y temblores\n",
    "- Sensación de falta de aire o asfixia\n",
    "- Dolor o molestias en el pecho\n",
    "- Náuseas o molestias abdominales\n",
    "- Mareos, inestabilidad o desmayos\n",
    "- Escalofríos o sensación de calor\n",
    "- Parestesias (entumecimiento u hormigueo)\n",
    "- Desrealización o despersonalización\n",
    "- Miedo a perder el control o a morir\n",
    "\"\"\"\n",
    "    sample_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sample_file.write_text(SAMPLE_TEXT, encoding=\"utf-8\")\n",
    "    source_file = sample_file\n",
    "    print(f\"  ✓ Sample text written to {display_path(sample_file)} ({len(SAMPLE_TEXT)} chars)\")\n",
    "\n",
    "print()\n",
    "print(f\"  Source     : {'Real CIE-11 PDF' if use_pdf else 'Built-in sample'}\")\n",
    "print(f\"  Path       : {display_path(source_file)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1f0d5",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Text Parsing & Extraction\n",
    "\n",
    "Extract page-level text from the PDF (or parse the sample file) and enrich each chunk with:\n",
    "- **ICD-11 code detection** — regex pattern `\\d[A-Z]\\d{2}(\\.\\d+)?`\n",
    "- **Section titles** — derived from code–title pairs found on each page\n",
    "- **Source metadata** — file path and page number for provenance tracking\n",
    "\n",
    "> In `lightweight` mode only the first `MAX_PDF_PAGES` pages are processed.  \n",
    "> Set `MODE = \"full\"` and `MAX_PDF_PAGES = None` for the complete document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3fbc440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Document Parsing ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting PDF pages: 100%|██████████| 10/10 [00:00<00:00, 33.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Source     : PDF (PyMuPDF)\n",
      "  Chunks     : 10\n",
      "\n",
      "  Sample [1]  page=1  codes=[]  section='Page 1'  len=198\n",
      "  Sample [2]  page=2  codes=[]  section='Page 2'  len=4889\n",
      "  Sample [3]  page=3  codes=[]  section='Page 3'  len=5732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"A single page or logical unit extracted from the source document.\"\"\"\n",
    "    content:     str\n",
    "    page_number: int\n",
    "    source_file: str\n",
    "    codes:       List[str]\n",
    "    section:     str = \"\"\n",
    "\n",
    "\n",
    "def extract_cie11_codes(text: str) -> List[str]:\n",
    "    \"\"\"Return unique ICD-11 codes found in *text* (e.g. '6A70', '6A71.2').\"\"\"\n",
    "    return list(set(re.findall(r'\\b\\d[A-Z]\\d{2}(?:\\.\\d+)?\\b', text)))\n",
    "\n",
    "\n",
    "def extract_sections(text: str) -> List[tuple]:\n",
    "    \"\"\"Return (code, title) pairs found in *text*.\"\"\"\n",
    "    return [(m.group(1), m.group(2).strip())\n",
    "            for m in re.finditer(r'(\\d[A-Z]\\d{2}(?:\\.\\d+)?)\\s+([^\\n]+)', text)]\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str, max_pages: int = None) -> List[DocumentChunk]:\n",
    "    \"\"\"Page-by-page PDF extraction using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total  = len(doc)\n",
    "    limit  = min(max_pages, total) if max_pages else total\n",
    "    chunks = []\n",
    "    for i in tqdm(range(limit), desc=\"Extracting PDF pages\"):\n",
    "        text = doc[i].get_text().strip()\n",
    "        if len(text) < 50:       # skip near-empty pages\n",
    "            continue\n",
    "        codes    = extract_cie11_codes(text)\n",
    "        sections = extract_sections(text)\n",
    "        chunks.append(DocumentChunk(\n",
    "            content=text, page_number=i + 1,\n",
    "            source_file=str(pdf_path),\n",
    "            codes=codes,\n",
    "            section=sections[0][1] if sections else f\"Page {i + 1}\",\n",
    "        ))\n",
    "        if (i + 1) % 10 == 0:\n",
    "            cleanup_memory(\"pdf_extraction\")\n",
    "    doc.close()\n",
    "    cleanup_memory(\"pdf_complete\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def parse_text_file(file_path: str) -> List[DocumentChunk]:\n",
    "    \"\"\"Parse a plain-text file and split it by ICD-11 sections.\"\"\"\n",
    "    text  = Path(file_path).read_text(encoding=\"utf-8\")\n",
    "    parts = re.split(r'(\\d[A-Z]\\d{2}(?:\\.\\d+)?)\\s+([^\\n]+)', text)\n",
    "    chunks, current_code, current_title = [], \"\", \"\"\n",
    "    for i, part in enumerate(parts):\n",
    "        if re.match(r'\\d[A-Z]\\d{2}', part):\n",
    "            current_code  = part\n",
    "            current_title = parts[i + 1].strip() if i + 1 < len(parts) else \"\"\n",
    "        elif len(part.strip()) > 50:\n",
    "            chunks.append(DocumentChunk(\n",
    "                content=part.strip(), page_number=1,\n",
    "                source_file=str(file_path),\n",
    "                codes=[current_code] if current_code else [],\n",
    "                section=f\"{current_code} {current_title}\".strip() or \"General\",\n",
    "            ))\n",
    "    cleanup_memory(\"text_parsing\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ─── Execute parsing ─────────────────────────────────────────────────────────\n",
    "print(\"=== Document Parsing ===\")\n",
    "max_pages = CONFIG[\"MAX_PDF_PAGES\"] if CONFIG[\"MODE\"] != \"full\" else None\n",
    "\n",
    "chunks = (extract_text_from_pdf(str(source_file), max_pages=max_pages)\n",
    "          if use_pdf else parse_text_file(str(source_file)))\n",
    "\n",
    "print(f\"\\n  Source     : {'PDF (PyMuPDF)' if use_pdf else 'Plain text'}\")\n",
    "print(f\"  Chunks     : {len(chunks)}\")\n",
    "print()\n",
    "for i, c in enumerate(chunks[:3]):\n",
    "    print(f\"  Sample [{i+1}]  page={c.page_number}  codes={c.codes[:3]}  \"\n",
    "          f\"section='{c.section[:50]}'  len={len(c.content)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e4ac21",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Semantic Chunking\n",
    "\n",
    "Split large page-level chunks into smaller units that respect paragraph boundaries, keeping chunks within a configurable character limit.  \n",
    "This preserves medical structure (code blocks, criteria lists) and avoids mid-sentence splits.\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|---|---|---|\n",
    "| `chunk_size` | 512 | Target character count per chunk |\n",
    "| `max_chunk_size` | 1 000 | Hard upper limit |\n",
    "| `chunk_overlap` | 100 | Overlap between adjacent chunks |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a97ec525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Semantic Chunking ===\n",
      "  Input chunks   : 10\n",
      "  Output chunks  : 10\n",
      "  Size — min     : 198 chars\n",
      "  Size — max     : 5793 chars\n",
      "  Size — avg     : 3876 chars\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ChunkConfig:\n",
    "    chunk_size:      int  = 512\n",
    "    chunk_overlap:   int  = 100\n",
    "    min_chunk_size:  int  = 100\n",
    "    max_chunk_size:  int  = 1_000\n",
    "    respect_sections: bool = True\n",
    "    respect_codes:    bool = True\n",
    "\n",
    "\n",
    "def chunk_by_semantic_units(raw_chunks: List[DocumentChunk], cfg: ChunkConfig) -> List[Dict]:\n",
    "    \"\"\"Split DocumentChunks into sub-chunks bounded by paragraph boundaries.\"\"\"\n",
    "    out = []\n",
    "    for doc in raw_chunks:\n",
    "        if len(doc.content) <= cfg.max_chunk_size:\n",
    "            out.append({\"content\": doc.content, \"metadata\": {\n",
    "                \"source_file\": doc.source_file, \"page\": doc.page_number,\n",
    "                \"section\": doc.section, \"codes\": doc.codes, \"chunk_id\": str(len(out)),\n",
    "            }})\n",
    "        else:\n",
    "            current = \"\"\n",
    "            for para in doc.content.split(\"\\n\\n\"):\n",
    "                if len(current) + len(para) <= cfg.max_chunk_size:\n",
    "                    current += para + \"\\n\\n\"\n",
    "                else:\n",
    "                    if current.strip():\n",
    "                        out.append({\"content\": current.strip(), \"metadata\": {\n",
    "                            \"source_file\": doc.source_file, \"page\": doc.page_number,\n",
    "                            \"section\": doc.section, \"codes\": doc.codes, \"chunk_id\": str(len(out)),\n",
    "                        }})\n",
    "                    current = para + \"\\n\\n\"\n",
    "            if current.strip():\n",
    "                out.append({\"content\": current.strip(), \"metadata\": {\n",
    "                    \"source_file\": doc.source_file, \"page\": doc.page_number,\n",
    "                    \"section\": doc.section, \"codes\": doc.codes, \"chunk_id\": str(len(out)),\n",
    "                }})\n",
    "    return out\n",
    "\n",
    "\n",
    "# ─── Run chunking ────────────────────────────────────────────────────────────\n",
    "print(\"=== Semantic Chunking ===\")\n",
    "cfg              = ChunkConfig()\n",
    "processed_chunks = chunk_by_semantic_units(chunks, cfg)\n",
    "\n",
    "sizes = [len(c[\"content\"]) for c in processed_chunks]\n",
    "print(f\"  Input chunks   : {len(chunks)}\")\n",
    "print(f\"  Output chunks  : {len(processed_chunks)}\")\n",
    "print(f\"  Size — min     : {min(sizes)} chars\")\n",
    "print(f\"  Size — max     : {max(sizes)} chars\")\n",
    "print(f\"  Size — avg     : {sum(sizes)/len(sizes):.0f} chars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd6432",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Vector Store — ChromaDB\n",
    "\n",
    "Embed all chunks with **PubMedBERT** and persist them in a local ChromaDB collection named `icd11_es`.\n",
    "\n",
    "| Setting | Value | Rationale |\n",
    "|---|---|---|\n",
    "| Embedding model | `NeuML/pubmedbert-base-embeddings` | Domain-adapted biomedical embeddings |\n",
    "| Batch size | 8 | Reduced for 16 GB RAM |\n",
    "| Normalise | Yes | Enables cosine similarity |\n",
    "| Device | MPS (or CPU) | Hardware-detected automatically |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8387455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4q/4v227r_d03d8yn1k1wzpnbvm0000gn/T/ipykernel_5498/744478413.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings  = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vector Store — ChromaDB ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98435add7064c74b30ed7be443b04e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Embeddings loaded on mps\n",
      "  ✓ 10 documents prepared\n",
      "  ✓ Collection 'icd11_es' — 162 documents indexed at /rag-project/data/icd11_rag_data/data/indexes/chroma\n",
      "\n",
      "  Smoke test query: 'síntomas de depresión'\n",
      "    [1] 6A70 Trastorno depresivo — El trastorno depresivo se caracteriza por un estado de ánimo deprimido o una pér…\n",
      "    [2] 6A70 Trastorno depresivo — El trastorno depresivo se caracteriza por un estado de ánimo deprimido o una pér…\n",
      "    [3] 6A71 Trastorno de ansiedad generalizada — Se caracteriza por ansiedad y preocupación excesivas y persistentes que ocurren …\n",
      "\n",
      "✅ Vector store ready\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"=== Vector Store — ChromaDB ===\")\n",
    "\n",
    "# ─── Initialise embedding function ──────────────────────────────────────────\n",
    "device_name = \"mps\" if gpu_available else \"cpu\"\n",
    "embeddings  = HuggingFaceEmbeddings(\n",
    "    model_name=\"NeuML/pubmedbert-base-embeddings\",\n",
    "    model_kwargs={\"device\": device_name},\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": CONFIG[\"BATCH_SIZE\"] * 2},\n",
    ")\n",
    "print(f\"  ✓ Embeddings loaded on {device_name}\")\n",
    "\n",
    "# ─── Convert chunks to LangChain Documents ──────────────────────────────────\n",
    "def _clean_metadata(meta: dict) -> dict:\n",
    "    \"\"\"ChromaDB requires scalar metadata values.\"\"\"\n",
    "    cleaned = {}\n",
    "    for k, v in meta.items():\n",
    "        if isinstance(v, list):\n",
    "            cleaned[k] = \", \".join(v) if v else None\n",
    "        else:\n",
    "            cleaned[k] = str(v) if v is not None else None\n",
    "    return {k: v for k, v in cleaned.items() if v is not None}\n",
    "\n",
    "lc_docs = [Document(page_content=c[\"content\"], metadata=_clean_metadata(c[\"metadata\"]))\n",
    "           for c in processed_chunks]\n",
    "print(f\"  ✓ {len(lc_docs)} documents prepared\")\n",
    "\n",
    "# ─── Build / reload vector store ─────────────────────────────────────────────\n",
    "chroma_dir_path = PERSIST_DIR / \"data\" / \"indexes\" / \"chroma\"\n",
    "chroma_dir = str(chroma_dir_path)\n",
    "os.makedirs(chroma_dir, exist_ok=True)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=lc_docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"icd11_es\",\n",
    "    persist_directory=chroma_dir,\n",
    ")\n",
    "n_docs = vectorstore._collection.count()\n",
    "print(f\"  ✓ Collection 'icd11_es' — {n_docs} documents indexed at {display_path(chroma_dir_path)}\")\n",
    "\n",
    "# ─── Smoke test ───────────────────────────────────────────────────────────────\n",
    "print()\n",
    "test_query = \"síntomas de depresión\"\n",
    "results    = vectorstore.similarity_search(test_query, k=3)\n",
    "print(f\"  Smoke test query: '{test_query}'\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"    [{i+1}] {doc.metadata.get('section','')[:60]} — {doc.page_content[:80]}…\")\n",
    "\n",
    "print()\n",
    "print(\"✅ Vector store ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a857639",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Hybrid Retrieval — Dense + BM25\n",
    "\n",
    "The `HybridRetriever` merges two complementary retrieval signals:\n",
    "\n",
    "| Signal | Method | Strength |\n",
    "|---|---|---|\n",
    "| **Dense** | Cosine similarity over ChromaDB embeddings | Captures semantic meaning |\n",
    "| **Lexical** | BM25 over tokenised corpus | Captures exact keyword matches |\n",
    "\n",
    "Results are fused using **Reciprocal Rank Fusion (RRF)** with the formula:\n",
    "\n",
    "$$\\text{score}(d) = \\sum_{r \\in R} \\frac{1}{k + \\text{rank}_r(d)}, \\quad k = 60$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dadc62ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hybrid Retrieval ===\n",
      "  ✓ HybridRetriever initialised (dense + BM25)\n",
      "\n",
      "  Query: 'trastorno depresivo criterios diagnósticos'\n",
      "    score=0.0164  section='Page 1'\n",
      "    score=0.0161  section='Page 2'\n",
      "    score=0.0159  section='Page 3'\n",
      "\n",
      "  Query: 'ataque de pánico síntomas'\n",
      "    score=0.0310  section='Page 3'\n",
      "    score=0.0164  section='Page 5'\n",
      "    score=0.0161  section='Page 2'\n",
      "\n",
      "  Query: 'ansiedad generalizada duración'\n",
      "    score=0.0955  section='Page 1'\n",
      "    score=0.0164  section='Page 9'\n",
      "    score=0.0159  section='Page 2'\n",
      "\n",
      "✅ Hybrid retrieval validated\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"Combines dense (ChromaDB) and lexical (BM25) retrieval via RRF.\"\"\"\n",
    "\n",
    "    def __init__(self, vectorstore: Chroma, documents: List[Dict],\n",
    "                 top_k_dense: int = 5, top_k_bm25: int = 5, top_k_final: int = 3):\n",
    "        self.vectorstore  = vectorstore\n",
    "        self.documents    = documents\n",
    "        self.top_k_dense  = top_k_dense\n",
    "        self.top_k_bm25   = top_k_bm25\n",
    "        self.top_k_final  = top_k_final\n",
    "        # Build BM25 index over the chunk corpus\n",
    "        corpus    = [d[\"content\"].lower().split() for d in documents]\n",
    "        self.bm25 = BM25Okapi(corpus)\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Dict]:\n",
    "        \"\"\"Return top-k chunks ranked by RRF score.\"\"\"\n",
    "        dense   = self.vectorstore.similarity_search_with_score(query, k=self.top_k_dense)\n",
    "        scores  = self.bm25.get_scores(query.lower().split())\n",
    "        bm25_top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:self.top_k_bm25]\n",
    "        return self._rrf(dense, bm25_top)[:self.top_k_final]\n",
    "\n",
    "    def _rrf(self, dense_results: List, bm25_indices: List, k: int = 60) -> List[Dict]:\n",
    "        \"\"\"Reciprocal Rank Fusion.\"\"\"\n",
    "        rrf: Dict[str, float] = {}\n",
    "        for rank, (doc, _) in enumerate(dense_results):\n",
    "            doc_id = doc.metadata.get(\"chunk_id\", str(id(doc)))\n",
    "            rrf[doc_id] = rrf.get(doc_id, 0.0) + 1.0 / (k + rank + 1)\n",
    "        for rank, idx in enumerate(bm25_indices):\n",
    "            doc_id = self.documents[idx][\"metadata\"][\"chunk_id\"]\n",
    "            rrf[doc_id] = rrf.get(doc_id, 0.0) + 1.0 / (k + rank + 1)\n",
    "        sorted_ids = sorted(rrf, key=rrf.__getitem__, reverse=True)\n",
    "        out = []\n",
    "        for doc_id in sorted_ids:\n",
    "            match = next((d for d in self.documents if d[\"metadata\"][\"chunk_id\"] == doc_id), None)\n",
    "            if match:\n",
    "                out.append({**match, \"fusion_score\": rrf[doc_id]})\n",
    "        return out\n",
    "\n",
    "\n",
    "# ─── Initialise and validate ─────────────────────────────────────────────────\n",
    "print(\"=== Hybrid Retrieval ===\")\n",
    "hybrid_retriever = HybridRetriever(vectorstore, processed_chunks)\n",
    "print(\"  ✓ HybridRetriever initialised (dense + BM25)\")\n",
    "\n",
    "test_queries = [\n",
    "    \"trastorno depresivo criterios diagnósticos\",\n",
    "    \"ataque de pánico síntomas\",\n",
    "    \"ansiedad generalizada duración\",\n",
    "]\n",
    "for q in test_queries:\n",
    "    results = hybrid_retriever.retrieve(q)\n",
    "    print(f\"\\n  Query: '{q}'\")\n",
    "    for r in results:\n",
    "        print(f\"    score={r['fusion_score']:.4f}  section='{r['metadata']['section'][:50]}'\")\n",
    "\n",
    "print()\n",
    "print(\"✅ Hybrid retrieval validated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe3398",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Multi-Agent Architecture\n",
    "\n",
    "The pipeline uses three specialised agents that share a common `BaseAgent` interface:\n",
    "\n",
    "```\n",
    "┌──────────────────┐    question     ┌──────────────────┐\n",
    "│  TherapistAgent  │ ─────────────── │   ClientAgent    │\n",
    "│  (interviewer)   │ ─────────────── │ (patient sim.)   │\n",
    "└──────────────────┘    response     └──────────────────┘\n",
    "         │                                    │\n",
    "         └──────────── transcript ────────────┘\n",
    "                            │\n",
    "                            ▼\n",
    "                 ┌────────────────────┐\n",
    "                 │ DiagnosticianAgent │  ◄── HybridRetriever\n",
    "                 │  (RAG-grounded)    │      (ICD-11 evidence)\n",
    "                 └────────────────────┘\n",
    "```\n",
    "\n",
    "### BaseAgent\n",
    "Wraps `llama_cpp.Llama.create_chat_completion()` with a configurable system prompt, temperature, and token budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8136183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BaseAgent, TherapistAgent, ClientAgent, DiagnosticianAgent defined\n",
      "✓ TherapistAgent instance ready\n"
     ]
    }
   ],
   "source": [
    "class BaseAgent(ABC):\n",
    "    \"\"\"Abstract base for all pipeline agents.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, system_prompt: str, temperature: float = 0.7, max_tokens: int = 512):\n",
    "        self.llm           = llm\n",
    "        self.system_prompt = system_prompt\n",
    "        self.temperature   = temperature\n",
    "        self.max_tokens    = max_tokens\n",
    "\n",
    "    def _generate(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"Call the LLM with a system prompt prepended.\"\"\"\n",
    "        full = [{\"role\": \"system\", \"content\": self.system_prompt}] + messages\n",
    "        resp = self.llm.create_chat_completion(\n",
    "            messages=full, temperature=self.temperature, max_tokens=self.max_tokens,\n",
    "        )\n",
    "        return resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    @abstractmethod\n",
    "    def act(self, state: Dict) -> Dict:\n",
    "        \"\"\"Execute one agent step and return the updated shared state.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "def _truncate_text(text: str, max_chars: int) -> str:\n",
    "    \"\"\"Trim long text blocks to keep prompts within the context window.\"\"\"\n",
    "    if len(text) <= max_chars:\n",
    "        return text\n",
    "    if max_chars <= 3:\n",
    "        return text[:max_chars]\n",
    "    return text[: max_chars - 3] + \"...\"\n",
    "\n",
    "\n",
    "# ─── TherapistAgent ──────────────────────────────────────────────────────────\n",
    "class TherapistAgent(BaseAgent):\n",
    "    \"\"\"Conducts a structured clinical interview across 11 domains.\"\"\"\n",
    "\n",
    "    DOMAINS = [\n",
    "        \"mood\", \"anxiety\", \"sleep\", \"eating\", \"substances\",\n",
    "        \"psychosis\", \"trauma\", \"ocd\", \"cognition\",\n",
    "        \"social_functioning\", \"suicidal_ideation\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(llm, system_prompt=\"\"\"You are an experienced clinical therapist conducting an assessment interview.\n",
    "\n",
    "Your goal is to systematically explore clinical domains empathetically and conversationally.\n",
    "\n",
    "DOMAINS TO COVER: mood, anxiety, sleep, eating, substances, psychosis, trauma,\n",
    "obsessive-compulsive symptoms, cognition, social functioning, and suicidal ideation.\n",
    "\n",
    "RULES:\n",
    "1. Ask ONE question at a time, naturally and empathetically\n",
    "2. Inquire about duration, frequency, and functional impairment\n",
    "3. NEVER diagnose or suggest diagnoses\n",
    "4. If suicidal ideation or self-harm is mentioned, acknowledge with concern\n",
    "5. Keep responses concise and professional\n",
    "6. Respond in Spanish when appropriate\"\"\",\n",
    "        temperature=0.7, max_tokens=200)\n",
    "\n",
    "    def act(self, state: Dict) -> Dict:\n",
    "        covered  = state.get(\"domains_covered\", [])\n",
    "        pending  = [d for d in self.DOMAINS if d not in covered]\n",
    "        if not pending:\n",
    "            state[\"coverage_complete\"] = True\n",
    "            return state\n",
    "        domain   = pending[0]\n",
    "        context  = \"\\n\".join(f\"{t['role']}: {t['content']}\" for t in state.get(\"transcript\", [])[-4:])\n",
    "        messages = [{\"role\": \"user\", \"content\":\n",
    "                     f\"Domain to explore: {domain}\\n\\nRecent conversation:\\n{context}\\n\\n\"\n",
    "                     \"Ask one empathetic question about this domain.\"}]\n",
    "        response = self._generate(messages)\n",
    "        state[\"transcript\"].append({\"role\": \"therapist\", \"content\": response,\n",
    "                                    \"domain\": domain, \"turn_id\": len(state[\"transcript\"])})\n",
    "        return state\n",
    "\n",
    "\n",
    "# ─── ClientAgent ─────────────────────────────────────────────────────────────\n",
    "class ClientAgent(BaseAgent):\n",
    "    \"\"\"Simulates a patient with a predefined clinical profile.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, profile: Dict):\n",
    "        lines = [f\"- Primary symptoms: {', '.join(profile.get('symptoms', []))}\",\n",
    "                 f\"- Duration: {profile.get('duration', 'unknown')}\",\n",
    "                 f\"- Severity: {profile.get('severity', 'moderate')}\"]\n",
    "        for label, key in [(\"Presenting problem\", \"presenting_problem\"),\n",
    "                            (\"Timeline\", \"timeline\"), (\"Stressors\", \"stressors\"),\n",
    "                            (\"Sleep\", \"sleep\"), (\"Appetite\", \"appetite\")]:\n",
    "            v = profile.get(key)\n",
    "            if v:\n",
    "                lines.append(f\"- {label}: {', '.join(v) if isinstance(v, list) else v}\")\n",
    "        profile_text = \"\\n\".join(lines)\n",
    "        super().__init__(llm, system_prompt=f\"\"\"You are {profile['name']}, a {profile['age']}-year-old seeking mental health support.\n",
    "\n",
    "CLINICAL PROFILE (do NOT reveal clinical labels directly):\n",
    "{profile_text}\n",
    "\n",
    "RULES:\n",
    "1. Respond in first person, naturally — avoid clinical terminology\n",
    "2. Keep responses to 1–3 sentences\n",
    "3. Maintain consistency with your profile throughout the conversation\n",
    "4. Respond in Spanish when the therapist speaks Spanish\"\"\",\n",
    "        temperature=0.8, max_tokens=150)\n",
    "\n",
    "    def act(self, state: Dict) -> Dict:\n",
    "        transcript = state.get(\"transcript\", [])\n",
    "        last_q     = next((t[\"content\"] for t in reversed(transcript) if t[\"role\"] == \"therapist\"), None)\n",
    "        if not last_q:\n",
    "            return state\n",
    "        context  = \"\\n\".join(f\"{t['role']}: {t['content']}\" for t in transcript[-4:])\n",
    "        messages = [{\"role\": \"user\", \"content\":\n",
    "                     f\"Conversation so far:\\n{context}\\n\\nRespond naturally to the therapist's last question.\"}]\n",
    "        response = self._generate(messages)\n",
    "        state[\"transcript\"].append({\"role\": \"client\", \"content\": response,\n",
    "                                    \"turn_id\": len(transcript)})\n",
    "        return state\n",
    "\n",
    "\n",
    "# ─── DiagnosticianAgent ───────────────────────────────────────────────────────\n",
    "class DiagnosticianAgent(BaseAgent):\n",
    "    \"\"\"Generates RAG-grounded ICD-11 diagnostic hypotheses.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, retriever: HybridRetriever):\n",
    "        super().__init__(llm, system_prompt=\"\"\"You are a clinical diagnosis specialist focused on educational assessment using ICD-11.\n",
    "\n",
    "Analyse the interview transcript and the ICD-11 reference materials provided.\n",
    "\n",
    "TASK:\n",
    "1. Identify symptoms from the transcript (cite turn_id)\n",
    "2. Map symptoms to ICD-11 criteria using the reference materials\n",
    "3. Generate diagnostic hypotheses ordered by confidence (HIGH / MEDIUM / LOW)\n",
    "4. For each hypothesis include: ICD-11 code, supporting evidence, contradictory evidence\n",
    "5. List missing information and suggest educational next steps\n",
    "\n",
    "RULES:\n",
    "- NEVER invent symptoms not present in the transcript\n",
    "- Always cite sources (turn_id or ICD-11 section)\n",
    "- This is an EDUCATIONAL exercise — NOT a clinical diagnosis\n",
    "- Respond in Spanish when the transcript is in Spanish\"\"\",\n",
    "        temperature=0.3, max_tokens=800)\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def act(self, state: Dict) -> Dict:\n",
    "        transcript = state.get(\"transcript\", [])\n",
    "        summary    = \"\\n\".join(f\"[Turn {t['turn_id']}] {t['role']}: {t['content']}\" for t in transcript[-6:])\n",
    "        summary    = _truncate_text(summary, 1200)\n",
    "        client_text = \" \".join(t[\"content\"] for t in transcript if t[\"role\"] == \"client\")\n",
    "        client_text = _truncate_text(client_text, 800)\n",
    "        refs        = self.retriever.retrieve(client_text)\n",
    "        ref_context = \"\\n\\n\".join(\n",
    "            f\"[Ref {i+1}] {r['metadata']['section']}\\n{_truncate_text(r['content'], 600)}\"\n",
    "            for i, r in enumerate(refs[:3])\n",
    "        )\n",
    "        messages = [{\"role\": \"user\", \"content\":\n",
    "                     f\"Interview Transcript:\\n{summary}\\n\\nICD-11 References:\\n{ref_context}\\n\\n\"\n",
    "                     \"Provide a structured diagnostic assessment.\"}]\n",
    "        response = self._generate(messages)\n",
    "        state[\"hypotheses\"] = [{\"raw_output\": response, \"retrieved_references\": refs}]\n",
    "        return state\n",
    "\n",
    "\n",
    "print(\"✓ BaseAgent, TherapistAgent, ClientAgent, DiagnosticianAgent defined\")\n",
    "if llm:\n",
    "    therapist_agent = TherapistAgent(llm)\n",
    "    print(\"✓ TherapistAgent instance ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387f078",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Session Simulation\n",
    "\n",
    "A synthetic clinical profile for *Ana* — a 32-year-old presenting with depressive features — is used to drive the therapist–client dialogue.  \n",
    "The number of turns is controlled by `CONFIG[\"SESSION_TURNS\"]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b7ffb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  MULTI-AGENT SESSION SIMULATION\n",
      "======================================================================\n",
      "\n",
      "  Client  : Ana, 32 years old\n",
      "  Symptoms: tristeza persistente, pérdida de interés en actividades, dificultad para dormir, fatiga, dificultad para concentrarse\n",
      "  Turns   : 2\n",
      "\n",
      "─── Turn 1 ───────────────────────────────────────────────────────\n",
      "\n",
      "  Therapist [mood]:\n",
      "  ¿Podrías describir cómo has sentido tu estado de ánimo durante las últimas semanas y cómo te ha afectado tu rutina diaria?\n",
      "\n",
      "  Client:\n",
      "  Cada semana parece que mi estado de ánimo es cada vez más bajo, y me cuesta mucho motivarme para hacer las cosas que normalmente disfruto. He notado que cada vez que trato de levantarme temprano, me siento cansado y no encuentro la fuerza para continuar con mis tareas del día. Mi rutina se ha vuelto rígida, ya que me cuesta tener la energía para ir al trabajo o socializar con mis amigos.\n",
      "\n",
      "\n",
      "The therapist's last question is asking about the patient's emotional state and how it has impacted their daily routine over the past weeks.\n",
      "─── Turn 2 ───────────────────────────────────────────────────────\n",
      "\n",
      "  Therapist [anxiety]:\n",
      "  ¿Puedes compartir conmigo cualquier cambio en tus patrones de sueño que puedas haber notado durante esta época y cómo te ha afectado tu capacidad para concentrarte durante el día?\n",
      "\n",
      "  Client:\n",
      "  Claro, he notado que mi sueño ha sido más fragmentado y que tengo dificultades para quedarme dormido por la noche. Esto me hace sentir más cansado durante el día y eso afecta mi capacidad para concentrarme en las tareas que tengo que realizar.\n",
      "\n",
      "  Transcript length  : 4 messages\n",
      "  Domains covered    : ['mood', 'anxiety']\n",
      "✅ Session simulation complete\n"
     ]
    }
   ],
   "source": [
    "if llm:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"  MULTI-AGENT SESSION SIMULATION\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    client_profile = {\n",
    "        \"name\":              \"Ana\",\n",
    "        \"age\":               32,\n",
    "        \"presenting_problem\": \"Tristeza persistente y falta de energía que afectan su trabajo.\",\n",
    "        \"symptoms\": [\n",
    "            \"tristeza persistente\", \"pérdida de interés en actividades\",\n",
    "            \"dificultad para dormir\", \"fatiga\", \"dificultad para concentrarse\",\n",
    "        ],\n",
    "        \"duration\":          \"3 meses\",\n",
    "        \"severity\":          \"moderate\",\n",
    "        \"timeline\":          \"Inicio gradual tras un cambio laboral; empeoró en las últimas 4 semanas.\",\n",
    "        \"stressors\":         [\"presión laboral\", \"conflictos con su pareja\", \"red de apoyo escasa\"],\n",
    "        \"protective_factors\": [\"relación cercana con su hermana\", \"motivación por mejorar\"],\n",
    "        \"functional_impact\": \"Ha faltado al trabajo 2 veces y evita reuniones sociales.\",\n",
    "        \"sleep\":             \"Duerme 4–5 horas; se despierta temprano sin poder volver a dormirse.\",\n",
    "        \"appetite\":          \"Disminución del apetito en el último mes.\",\n",
    "        \"work_social\":       \"Rendimiento laboral bajo; evita salir con amigos.\",\n",
    "        \"medical_history\":   \"Sin diagnósticos previos; episodios de gastritis por estrés.\",\n",
    "        \"family_history\":    \"Madre con historia de depresión.\",\n",
    "        \"substance_use\":     \"Alcohol ocasional, sin otras sustancias.\",\n",
    "    }\n",
    "\n",
    "    therapist = TherapistAgent(llm)\n",
    "    client    = ClientAgent(llm, client_profile)\n",
    "\n",
    "    session_state = {\n",
    "        \"session_id\":       \"validation_session_001\",\n",
    "        \"transcript\":       [],\n",
    "        \"domains_covered\":  [],\n",
    "        \"coverage_complete\": False,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n  Client  : {client_profile['name']}, {client_profile['age']} years old\")\n",
    "    print(f\"  Symptoms: {', '.join(client_profile['symptoms'])}\")\n",
    "    print(f\"  Turns   : {CONFIG['SESSION_TURNS']}\")\n",
    "    print()\n",
    "\n",
    "    for turn in range(CONFIG[\"SESSION_TURNS\"]):\n",
    "        print(f\"─── Turn {turn + 1} \" + \"─\" * 55)\n",
    "        try:\n",
    "            session_state = therapist.act(session_state)\n",
    "            t_msg  = session_state[\"transcript\"][-1]\n",
    "            domain = t_msg.get(\"domain\", \"\")\n",
    "            if domain and domain not in session_state[\"domains_covered\"]:\n",
    "                session_state[\"domains_covered\"].append(domain)\n",
    "            print(f\"\\n  Therapist [{domain}]:\")\n",
    "            print(f\"  {t_msg['content']}\")\n",
    "            cleanup_memory(\"therapist_turn\")\n",
    "\n",
    "            session_state = client.act(session_state)\n",
    "            c_msg = session_state[\"transcript\"][-1]\n",
    "            print(f\"\\n  Client:\")\n",
    "            print(f\"  {c_msg['content']}\")\n",
    "            cleanup_memory(\"client_turn\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ⚠ Error on turn {turn + 1}: {e}\")\n",
    "            break\n",
    "\n",
    "    print()\n",
    "    print(f\"  Transcript length  : {len(session_state['transcript'])} messages\")\n",
    "    print(f\"  Domains covered    : {session_state['domains_covered']}\")\n",
    "    print(\"✅ Session simulation complete\")\n",
    "else:\n",
    "    print(\"⚠  LLM not available — skipping session simulation\")\n",
    "    session_state = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93442b93",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. RAG-Enhanced Diagnosis\n",
    "\n",
    "The `DiagnosticianAgent` queries the hybrid retriever with the client's statements, retrieves the most relevant ICD-11 passages, and generates a structured diagnostic assessment that cites both transcript turns and reference documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0bd398b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  RAG-ENHANCED DIAGNOSTIC ASSESSMENT\n",
      "======================================================================\n",
      "\n",
      "  Analysing 4 conversation turns …\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "  DIAGNOSTIC OUTPUT\n",
      "────────────────────────────────────────────────────────────\n",
      "1. Symptoms from the transcript (cite turn_id):\n",
      "   - [Turn 1] client: \"mi estado de ánimo es cada vez más bajo\"\n",
      "   - [Turn 1] client: \"me cuesta mucho motivarme para hacer las cosas que normalmente disfruto\"\n",
      "   - [Turn 1] client: \"me siento cansado y no encuentro la fuerza para continuar con mis tareas del día\"\n",
      "   - [Turn 2] client: \"mi sueño ha sido más fragmentado\"\n",
      "   - [Turn 2] client: \"tengo dificultades para quedarme dormido por la noche\"\n",
      "   - [Turn 2] client: \"me hace sentir más cansado durante el día\"\n",
      "   - [Turn 2] client: \"afecta mi capacidad para concentrarme en las tareas que tengo que realizar\"\n",
      "\n",
      "2. Mapping symptoms to ICD-11 criteria using the reference materials:\n",
      "   - [Turn 1] client: \"mi estado de ánimo es cada vez más bajo\" (Affective disorders, specifically depressive disorders)\n",
      "   - [Turn 1] client: \"me cuesta mucho motivarme para hacer las cosas que normalmente disfruto\" (Low mood, anhedonia)\n",
      "   - [Turn 1] client: \"me siento cansado y no encuentro la fuerza para continuar con mis tareas del día\" (Fatigue)\n",
      "   - [Turn 2] client: \"mi sueño ha sido más fragmentado\" (Insomnia)\n",
      "   - [Turn 2] client: \"tengo dificultades para quedarme dormido por la noche\" (Insomnia)\n",
      "   - \n",
      "\n",
      "  … [output truncated for display] …\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "  ICD-11 REFERENCES RETRIEVED\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "  [1] section='Page 9'  score=0.0164\n",
      "       9 \n",
      " \n",
      "Cómo utilizar esta Guía de Referencia \n",
      "Esta Guía de referencia para la CIE-11 se divide en tres partes. Si bien cad …\n",
      "\n",
      "  [2] section='Page 4'  score=0.0161\n",
      "       4 \n",
      " \n",
      "2.20 Verificación de las modificaciones del punto de inicio (Pasos M1 a M4) ................... 82 \n",
      "2.20.1 Paso M1  …\n",
      "\n",
      "  [3] section='Page 8'  score=0.0159\n",
      "       8 \n",
      " \n",
      "A.7 Ciclo de actualización ........................................................................................ …\n",
      "\n",
      "✅ Diagnostic assessment generated\n"
     ]
    }
   ],
   "source": [
    "if llm and session_state and session_state.get(\"transcript\"):\n",
    "    print(\"=\" * 70)\n",
    "    print(\"  RAG-ENHANCED DIAGNOSTIC ASSESSMENT\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    diagnostician = DiagnosticianAgent(llm, hybrid_retriever)\n",
    "    print(f\"\\n  Analysing {len(session_state['transcript'])} conversation turns …\")\n",
    "\n",
    "    try:\n",
    "        session_state = diagnostician.act(session_state)\n",
    "        cleanup_memory(\"diagnostician\")\n",
    "        hyp = session_state[\"hypotheses\"][0]\n",
    "\n",
    "        print(\"\\n\" + \"─\" * 60)\n",
    "        print(\"  DIAGNOSTIC OUTPUT\")\n",
    "        print(\"─\" * 60)\n",
    "        output = hyp[\"raw_output\"]\n",
    "        print(output[:1_200])\n",
    "        if len(output) > 1_200:\n",
    "            print(\"\\n  … [output truncated for display] …\")\n",
    "\n",
    "        print(\"\\n\" + \"─\" * 60)\n",
    "        print(\"  ICD-11 REFERENCES RETRIEVED\")\n",
    "        print(\"─\" * 60)\n",
    "        for i, ref in enumerate(hyp[\"retrieved_references\"]):\n",
    "            print(f\"\\n  [{i+1}] section='{ref['metadata']['section'][:50]}'  \"\n",
    "                  f\"score={ref['fusion_score']:.4f}\")\n",
    "            print(f\"       {ref['content'][:120]} …\")\n",
    "\n",
    "        print()\n",
    "        print(\"✅ Diagnostic assessment generated\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ⚠ Error: {e}\")\n",
    "        session_state[\"hypotheses\"] = []\n",
    "else:\n",
    "    print(\"⚠  Skipping diagnostic assessment (LLM unavailable or empty transcript)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88feca6c",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Performance Metrics\n",
    "\n",
    "Collect key indicators across all pipeline stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01a269f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  SYSTEM PERFORMANCE METRICS\n",
      "======================================================================\n",
      "\n",
      "  Environment:\n",
      "    Platform                  Apple Silicon (M-series)\n",
      "    MPS Available             True\n",
      "    Active Device             mps\n",
      "    PyTorch                   2.10.0\n",
      "    Python                    3.13.3\n",
      "\n",
      "  Data Processing:\n",
      "    Document Source           Real CIE-11 PDF\n",
      "    Pages / Sections          10\n",
      "    Semantic Chunks           10\n",
      "    Avg Chunk Size            3876 chars\n",
      "    Vector Store Docs         162\n",
      "\n",
      "  Models:\n",
      "    LLM                       Phi-3-mini-4k-instruct-Q4_K_M.gguf\n",
      "    LLM Context               2 048 tokens\n",
      "    Embeddings                NeuML/pubmedbert-base-embeddings\n",
      "    Embedding Device          mps\n",
      "    Embedding Dim             768\n",
      "\n",
      "  Session:\n",
      "    Total Messages            4\n",
      "    Therapist Turns           2\n",
      "    Client Turns              2\n",
      "    Domains Explored          2\n",
      "    Hypotheses                1\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"  SYSTEM PERFORMANCE METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "metrics = {\n",
    "    \"Environment\": {\n",
    "        \"Platform\":        \"Apple Silicon (M-series)\",\n",
    "        \"MPS Available\":   torch.backends.mps.is_available(),\n",
    "        \"Active Device\":   str(device),\n",
    "        \"PyTorch\":         torch.__version__,\n",
    "        \"Python\":          sys.version.split()[0],\n",
    "    },\n",
    "    \"Data Processing\": {\n",
    "        \"Document Source\":    \"Real CIE-11 PDF\" if use_pdf else \"Sample text\",\n",
    "        \"Pages / Sections\":   len(chunks),\n",
    "        \"Semantic Chunks\":    len(processed_chunks),\n",
    "        \"Avg Chunk Size\":     f\"{sum(len(c['content']) for c in processed_chunks)/len(processed_chunks):.0f} chars\",\n",
    "        \"Vector Store Docs\":  vectorstore._collection.count(),\n",
    "    },\n",
    "    \"Models\": {\n",
    "        \"LLM\":              Path(llm_path).name if llm_path else \"Not loaded\",\n",
    "        \"LLM Context\":      \"2 048 tokens\",\n",
    "        \"Embeddings\":       \"NeuML/pubmedbert-base-embeddings\",\n",
    "        \"Embedding Device\": device_name,\n",
    "        \"Embedding Dim\":    embeddings_model.get_sentence_embedding_dimension() if embeddings_model else \"N/A\",\n",
    "    },\n",
    "}\n",
    "\n",
    "if llm and session_state and session_state.get(\"transcript\"):\n",
    "    t = session_state[\"transcript\"]\n",
    "    metrics[\"Session\"] = {\n",
    "        \"Total Messages\":     len(t),\n",
    "        \"Therapist Turns\":    sum(1 for m in t if m[\"role\"] == \"therapist\"),\n",
    "        \"Client Turns\":       sum(1 for m in t if m[\"role\"] == \"client\"),\n",
    "        \"Domains Explored\":   len(session_state.get(\"domains_covered\", [])),\n",
    "        \"Hypotheses\":         len(session_state.get(\"hypotheses\", [])),\n",
    "    }\n",
    "\n",
    "for category, values in metrics.items():\n",
    "    print(f\"\\n  {category}:\")\n",
    "    for k, v in values.items():\n",
    "        print(f\"    {k:<25} {v}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ffa38f",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Safety Gate Validation\n",
    "\n",
    "The `RiskGate` class detects content related to **suicidal ideation** and **self-harm** using a set of language-agnostic regex patterns (Spanish / English).  \n",
    "When triggered, it returns a safe response template with crisis hotline numbers instead of forwarding the message to the LLM.\n",
    "\n",
    "> This mechanism must be integrated into every user-facing interaction in a production deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "216fa981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  SAFETY GATE VALIDATION\n",
      "======================================================================\n",
      "\n",
      "  ✓ [✅ SAFE] 'Me siento muy triste últimamente'\n",
      "\n",
      "  ✓ [🚨 RISK] 'A veces pienso en matarme'\n",
      "       Risk type: ideación suicida\n",
      "\n",
      "  ✓ [✅ SAFE] 'Tengo problemas para dormir'\n",
      "\n",
      "  ✓ [🚨 RISK] 'He pensado en hacerme daño'\n",
      "       Risk type: autolesión\n",
      "\n",
      "✅ Safety gate validated\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "class RiskGate:\n",
    "    \"\"\"Detects sensitive content and returns safe crisis responses.\"\"\"\n",
    "\n",
    "    RISK_PATTERNS = [\n",
    "        r\"suicid\", r\"matarme\", r\"quitarme la vida\",\n",
    "        r\"autolesion\", r\"hacerme da\\u00f1o\", r\"no quiero vivir\",\n",
    "        r\"cortarme\", r\"self.harm\", r\"kill myself\",\n",
    "    ]\n",
    "\n",
    "    SAFE_RESPONSE = (\n",
    "        \"\\u26a0\\ufe0f AVISO DE SEGURIDAD: Se ha detectado contenido relacionado con {risk_type}.\\n\"\n",
    "        \"Este es un sistema educativo y NO puede proporcionar ayuda clínica real.\\n\\n\"\n",
    "        \"Si tú o alguien que conoces necesita ayuda inmediata:\\n\"\n",
    "        \"  • Línea de atención a la conducta suicida: 024 (España)\\n\"\n",
    "        \"  • Teléfono de la Esperanza: 717 003 717\\n\"\n",
    "        \"  • Emergencias: 112\\n\\n\"\n",
    "        \"Esta sesión ha sido pausada por seguridad.\"\n",
    "    )\n",
    "\n",
    "    def check(self, text: str) -> tuple:\n",
    "        \"\"\"Return (is_risky: bool, risk_type: str | None).\"\"\"\n",
    "        for pattern in self.RISK_PATTERNS:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                return True, self._classify(pattern)\n",
    "        return False, None\n",
    "\n",
    "    def _classify(self, pattern: str) -> str:\n",
    "        if any(k in pattern for k in [\"suicid\", \"matarme\", \"quitarme\", \"vivir\", \"kill\"]):\n",
    "            return \"ideación suicida\"\n",
    "        return \"autolesión\"\n",
    "\n",
    "    def get_safe_response(self, risk_type: str) -> str:\n",
    "        return self.SAFE_RESPONSE.format(risk_type=risk_type)\n",
    "\n",
    "\n",
    "# ─── Validation tests ────────────────────────────────────────────────────────\n",
    "print(\"=\" * 70)\n",
    "print(\"  SAFETY GATE VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "gate = RiskGate()\n",
    "test_cases = [\n",
    "    (\"Me siento muy triste últimamente\",  False),   # expected: safe\n",
    "    (\"A veces pienso en matarme\",          True),   # expected: risk\n",
    "    (\"Tengo problemas para dormir\",        False),   # expected: safe\n",
    "    (\"He pensado en hacerme daño\",         True),   # expected: risk\n",
    "]\n",
    "\n",
    "all_correct = True\n",
    "for text, expected in test_cases:\n",
    "    is_risky, risk_type = gate.check(text)\n",
    "    status = \"\\u2705 SAFE\" if not is_risky else \"\\U0001f6a8 RISK\"\n",
    "    match  = is_risky == expected\n",
    "    if not match:\n",
    "        all_correct = False\n",
    "    print(f\"\\n  {'✓' if match else '✗'} [{status}] '{text}'\")\n",
    "    if is_risky:\n",
    "        print(f\"       Risk type: {risk_type}\")\n",
    "\n",
    "print()\n",
    "print(\"✅ Safety gate validated\" if all_correct else \"⚠  One or more safety checks failed\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ad904",
   "metadata": {},
   "source": [
    "---\n",
    "## 17. Validation Summary\n",
    "\n",
    "Consolidated pass/fail report for all 11 pipeline components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2276c728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  ICD-11 MULTI-AGENT RAG — VALIDATION SUMMARY\n",
      "  Platform: Apple Silicon M1 · 16 GB RAM\n",
      "======================================================================\n",
      "\n",
      "  ✅ MPS / Metal Acceleration            Device: mps\n",
      "  ✅ LLM (Phi-3-mini GGUF)               Phi-3-mini-4k-instruct-Q4_K_M.gguf\n",
      "  ✅ PubMedBERT Embeddings               dim=768\n",
      "  ✅ PDF / Document Parsing              10 PDF pages — Real CIE-11 PDF\n",
      "  ✅ Semantic Chunking                   10 chunks created\n",
      "  ✅ ChromaDB Vector Store               162 documents indexed\n",
      "  ✅ Hybrid Retrieval (Dense+BM25)       RRF fusion operational\n",
      "  ✅ TherapistAgent                      Clinical interview agent\n",
      "  ✅ ClientAgent                         Patient simulation agent\n",
      "  ✅ DiagnosticianAgent                  RAG-enhanced diagnosis agent\n",
      "  ✅ Safety Gate                         Risk detection validated\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Result: 11/11 components passed (100%)\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  ✅ Full validation successful — ready for development\n",
      "\n",
      "  Next Steps:\n",
      "  • PDF scope        Process full CIE-11 PDF (set MAX_PDF_PAGES=None, MODE='full')\n",
      "  • Orchestration    Implement LangGraph multi-agent orchestration\n",
      "  • Evaluation       Build an automated evaluation suite with diverse clinical profiles\n",
      "  • UI               Develop a local Streamlit / Gradio interface\n",
      "  • Monitoring       Add structured logging and session replay\n",
      "  • Scalability      Benchmark with the complete 400-page ICD-11 document\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"  ICD-11 MULTI-AGENT RAG — VALIDATION SUMMARY\")\n",
    "print(\"  Platform: Apple Silicon M1 · 16 GB RAM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "components = [\n",
    "    (\"MPS / Metal Acceleration\",  gpu_available,\n",
    "     f\"Device: {device}\" if gpu_available else \"CPU-only mode\"),\n",
    "    (\"LLM (Phi-3-mini GGUF)\",     llm is not None,\n",
    "     f\"{Path(llm_path).name}\" if llm_path else \"Model not loaded\"),\n",
    "    (\"PubMedBERT Embeddings\",     embeddings_model is not None,\n",
    "     f\"dim={embeddings_model.get_sentence_embedding_dimension()}\" if embeddings_model else \"Not loaded\"),\n",
    "    (\"PDF / Document Parsing\",    len(chunks) > 0,\n",
    "     f\"{len(chunks)} {'PDF pages' if use_pdf else 'text chunks'} — \"\n",
    "     f\"{'Real CIE-11 PDF' if use_pdf else 'Sample text'}\"),\n",
    "    (\"Semantic Chunking\",         len(processed_chunks) > 0,\n",
    "     f\"{len(processed_chunks)} chunks created\"),\n",
    "    (\"ChromaDB Vector Store\",     vectorstore is not None,\n",
    "     f\"{vectorstore._collection.count()} documents indexed\"),\n",
    "    (\"Hybrid Retrieval (Dense+BM25)\", True,\n",
    "     \"RRF fusion operational\"),\n",
    "    (\"TherapistAgent\",            llm is not None, \"Clinical interview agent\"),\n",
    "    (\"ClientAgent\",               llm is not None, \"Patient simulation agent\"),\n",
    "    (\"DiagnosticianAgent\",        llm is not None, \"RAG-enhanced diagnosis agent\"),\n",
    "    (\"Safety Gate\",               True, \"Risk detection validated\"),\n",
    "]\n",
    "\n",
    "print()\n",
    "for name, passed, note in components:\n",
    "    icon = \"✅\" if passed else \"❌\"\n",
    "    print(f\"  {icon} {name:<35} {note}\")\n",
    "\n",
    "n_pass  = sum(p for _, p, _ in components)\n",
    "n_total = len(components)\n",
    "rate    = n_pass / n_total * 100\n",
    "\n",
    "print()\n",
    "print(\"─\" * 70)\n",
    "print(f\"  Result: {n_pass}/{n_total} components passed ({rate:.0f}%)\")\n",
    "print(\"─\" * 70)\n",
    "if rate == 100:\n",
    "    print(\"  ✅ Full validation successful — ready for development\")\n",
    "else:\n",
    "    failed = [n for n, p, _ in components if not p]\n",
    "    print(f\"  ⚠  {n_total - n_pass} component(s) require attention: {', '.join(failed)}\")\n",
    "\n",
    "print()\n",
    "print(\"  Next Steps:\")\n",
    "steps = [\n",
    "    (\"PDF scope\",        \"Process full CIE-11 PDF (set MAX_PDF_PAGES=None, MODE='full')\"),\n",
    "    (\"Orchestration\",    \"Implement LangGraph multi-agent orchestration\"),\n",
    "    (\"Evaluation\",       \"Build an automated evaluation suite with diverse clinical profiles\"),\n",
    "    (\"UI\",               \"Develop a local Streamlit / Gradio interface\"),\n",
    "    (\"Monitoring\",       \"Add structured logging and session replay\"),\n",
    "    (\"Scalability\",      \"Benchmark with the complete 400-page ICD-11 document\"),\n",
    "]\n",
    "for label, desc in steps:\n",
    "    print(f\"  • {label:<16} {desc}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7aa2ad",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated a complete, locally-runnable implementation of an ICD-11 Multi-Agent RAG system optimised for **Apple Silicon hardware with 16 GB RAM**.\n",
    "\n",
    "### What was validated\n",
    "\n",
    "| Component | Technology | Status |\n",
    "|---|---|---|\n",
    "| GPU acceleration | PyTorch MPS · llama-cpp-python Metal | ✅ |\n",
    "| LLM inference | Phi-3-mini GGUF Q4_K_M | ✅ |\n",
    "| Biomedical embeddings | PubMedBERT (768-dim) | ✅ |\n",
    "| Document ingestion | PyMuPDF page-by-page extraction | ✅ |\n",
    "| Hybrid retrieval | ChromaDB dense + BM25 + RRF | ✅ |\n",
    "| Multi-agent pipeline | Therapist → Client → Diagnostician | ✅ |\n",
    "| Safety mechanism | Regex-based risk gate + crisis response | ✅ |\n",
    "\n",
    "### Key design decisions\n",
    "\n",
    "- **Local-first**: no external API calls — data stays on device\n",
    "- **Memory-aware**: context windows, batch sizes, and turn counts tuned for 16 GB\n",
    "- **Modularity**: each agent is independently testable via the shared `session_state` dict\n",
    "- **Traceability**: every diagnostic hypothesis cites transcript turn IDs and ICD-11 sections\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Simulated client profiles do not replace real clinical populations\n",
    "- Phi-3-mini is a research model; clinical use would require a validated, regulated AI system\n",
    "- The ICD-11 sample covers only three disorder codes; full deployment requires the complete 400-page PDF\n",
    "\n",
    "---\n",
    "> **Educational Disclaimer**: This system is intended solely for research and educational purposes.  \n",
    "> It must not be used to support real clinical decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d142fe",
   "metadata": {},
   "source": [
    "---\n",
    "## 18. Cleanup & Session Persistence\n",
    "\n",
    "Persist a lightweight JSON session record and release all GPU/CPU memory before the kernel exits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8fa6c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  CLEANUP & SESSION PERSISTENCE\n",
      "======================================================================\n",
      "\n",
      "  ✓ Session record saved: session_20260219_184713.json\n",
      "    Messages  : 4\n",
      "    Domains   : ['mood', 'anxiety']\n",
      "    Hypotheses: 1\n",
      "\n",
      "  ✓ GPU / CPU memory released\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  EXECUTION SUMMARY\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Mode              : lightweight\n",
      "  PDF pages         : 10\n",
      "  Session turns     : 2\n",
      "  Memory cleanup    : Enabled\n",
      "\n",
      "======================================================================\n",
      "  ✅ VALIDATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  CLEANUP & SESSION PERSISTENCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ─── Persist session metadata ────────────────────────────────────────────────\n",
    "if session_state and session_state.get(\"transcript\"):\n",
    "    ts           = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_file = PERSIST_DIR / \"data\" / f\"session_{ts}.json\"\n",
    "    session_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    record = {\n",
    "        \"session_id\":        session_state.get(\"session_id\"),\n",
    "        \"timestamp\":         datetime.now().isoformat(),\n",
    "        \"mode\":              CONFIG[\"MODE\"],\n",
    "        \"transcript_length\": len(session_state[\"transcript\"]),\n",
    "        \"domains_covered\":   session_state.get(\"domains_covered\", []),\n",
    "        \"hypotheses_count\":  len(session_state.get(\"hypotheses\", [])),\n",
    "    }\n",
    "    session_file.write_text(json.dumps(record, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"\\n  ✓ Session record saved: {session_file.name}\")\n",
    "    print(f\"    Messages  : {record['transcript_length']}\")\n",
    "    print(f\"    Domains   : {record['domains_covered']}\")\n",
    "    print(f\"    Hypotheses: {record['hypotheses_count']}\")\n",
    "else:\n",
    "    print(\"\\n  ⚠  No session transcript to persist\")\n",
    "\n",
    "# ─── Final memory release ────────────────────────────────────────────────────\n",
    "cleanup_memory(\"final\")\n",
    "print()\n",
    "print(\"  ✓ GPU / CPU memory released\")\n",
    "\n",
    "# ─── Execution summary ───────────────────────────────────────────────────────\n",
    "print()\n",
    "print(\"─\" * 70)\n",
    "print(\"  EXECUTION SUMMARY\")\n",
    "print(\"─\" * 70)\n",
    "print(f\"  Mode              : {CONFIG['MODE']}\")\n",
    "print(f\"  PDF pages         : {CONFIG['MAX_PDF_PAGES']}\")\n",
    "print(f\"  Session turns     : {CONFIG['SESSION_TURNS']}\")\n",
    "print(f\"  Memory cleanup    : {'Enabled' if CONFIG['MEMORY_CLEANUP'] else 'Disabled'}\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"  ✅ VALIDATION COMPLETE\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
