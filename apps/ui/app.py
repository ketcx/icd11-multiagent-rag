"""Streamlit UI application for ICD-11 Multi-Agent RAG."""

from __future__ import annotations

import time
import uuid
from pathlib import Path

import streamlit as st
import yaml

from core.orchestration.graph import build_graph
from core.orchestration.nodes import AGENTS

# ---------------------------------------------------------------------------
# Internationalisation (i18n)
# ---------------------------------------------------------------------------

_STRINGS: dict[str, dict[str, str]] = {
    # Sidebar
    "sidebar_header": {"en": "âš™ï¸ Configuration", "es": "âš™ï¸ ConfiguraciÃ³n"},
    "llm_loaded": {"en": "âœ… Model loaded", "es": "âœ… Modelo cargado"},
    "llm_mock": {"en": "âš ï¸ No model (mock mode)", "es": "âš ï¸ Sin modelo (modo mock)"},
    "language_label": {"en": "Language / Idioma", "es": "Idioma / Language"},
    "mode_label": {"en": "Patient Mode", "es": "Modo de Paciente"},
    "mode_auto": {"en": "Auto (Simulated Profile)", "es": "Auto (Perfil Simulado)"},
    "mode_interactive": {"en": "Interactive (Human)", "es": "Interactivo (Humano)"},
    "max_turns_caption": {"en": "max turns", "es": "turnos mÃ¡x."},
    "domains_caption": {"en": "domains", "es": "dominios"},
    "new_session_btn": {"en": "ðŸ”„ New Session", "es": "ðŸ”„ Nueva SesiÃ³n"},
    # Page title / header
    "page_title": {"en": "ICD-11 Multi-Agent RAG", "es": "ICD-11 Multi-Agent RAG"},
    "page_subtitle": {
        "en": "Educational clinical interview simulator",
        "es": "Simulador educativo de entrevista clÃ­nica",
    },
    "disclaimer": {
        "en": (
            "âš ï¸ **NOTICE:** This is an educational system only. "
            "It does **not** provide real clinical diagnosis or medical recommendations. "
            "Generated hypotheses are simulations for research purposes."
        ),
        "es": (
            "âš ï¸ **AVISO:** Este es un sistema exclusivamente educativo. "
            "**No** proporciona diagnÃ³stico clÃ­nico real. "
            "Las hipÃ³tesis generadas son simulaciones para fines de investigaciÃ³n."
        ),
    },
    # Session control
    "btn_start_interactive": {
        "en": "â–¶ï¸ Start Interactive Interview",
        "es": "â–¶ï¸ Iniciar Entrevista Interactiva",
    },
    "btn_start_auto": {
        "en": "â–¶ï¸ Start Automatic Simulation",
        "es": "â–¶ï¸ Iniciar SimulaciÃ³n AutomÃ¡tica",
    },
    "spinner_init": {"en": "Initialising sessionâ€¦", "es": "Inicializando sesiÃ³nâ€¦"},
    "spinner_thinking": {"en": "Therapist is thinkingâ€¦", "es": "El terapeuta estÃ¡ pensandoâ€¦"},
    "spinner_simulating": {"en": "Simulating next turnâ€¦", "es": "Simulando siguiente turnoâ€¦"},
    "chat_placeholder": {"en": "Type your response hereâ€¦", "es": "Escribe tu respuesta aquÃ­â€¦"},
    # Progress info
    "progress_info": {
        "en": "Turn {turn}/{max} Â· Pending domains: {pending} Â· Covered: {covered}",
        "es": "Turno {turn}/{max} Â· Dominios pendientes: {pending} Â· Cubiertos: {covered}",
    },
    # Results
    "session_complete": {"en": "âœ… Session complete", "es": "âœ… SesiÃ³n completada"},
    "risk_terminated": {
        "en": "Session terminated by safety gate.",
        "es": "SesiÃ³n terminada por la puerta de seguridad.",
    },
    "hypotheses_header": {"en": "Diagnostic Hypotheses", "es": "HipÃ³tesis DiagnÃ³sticas"},
    "no_hypotheses": {"en": "No hypotheses generated.", "es": "Sin hipÃ³tesis generadas."},
    "confidence_label": {"en": "Confidence", "es": "Confianza"},
    "evidence_for_label": {"en": "Supporting evidence", "es": "Evidencia a favor"},
    "evidence_against_label": {"en": "Contradicting evidence", "es": "Evidencia en contra"},
    "audit_header": {"en": "Evidence Audit", "es": "AuditorÃ­a de Evidencia"},
    "traceability_metric": {"en": "Traceability score", "es": "PuntuaciÃ³n de trazabilidad"},
    "issues_warning": {
        "en": "{n} claim(s) without direct grounding",
        "es": "{n} afirmaciÃ³n(es) sin respaldo directo",
    },
    "no_issues": {"en": "All claims traceable", "es": "Todas las afirmaciones trazables"},
    "auditor_comment": {"en": "Auditor commentary", "es": "Comentario del auditor"},
    "full_transcript": {"en": "Full Transcript", "es": "TranscripciÃ³n Completa"},
    "view_transcript": {"en": "View transcript", "es": "Ver transcripciÃ³n"},
    "domain_caption": {"en": "domain", "es": "dominio"},
    # Model loading spinner (must be a static string for @st.cache_resource)
    "spinner_loading_model": {"en": "Loading LLM into memoryâ€¦", "es": "Cargando modelo LLMâ€¦"},
}


def t(key: str, lang: str | None = None, **kwargs: object) -> str:
    """Returns the UI string for *key* in the current session language.

    Args:
        key:    Translation key defined in ``_STRINGS``.
        lang:   Language override ("en"/"es"). Uses session language when omitted.
        **kwargs: Format arguments forwarded to ``str.format``.
    """
    if lang is None:
        raw = st.session_state.get("session_language", "English")
        lang = "es" if raw == "EspaÃ±ol" else "en"
    text = _STRINGS.get(key, {}).get(lang, _STRINGS.get(key, {}).get("en", key))
    return text.format(**kwargs) if kwargs else text


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------


@st.cache_resource(show_spinner=False)
def _load_config() -> dict:
    cfg_path = Path(__file__).parent.parent.parent / "configs" / "app.yaml"
    with open(cfg_path) as fh:
        return yaml.safe_load(fh)


# ---------------------------------------------------------------------------
# Agent / model loading (with graceful mock fallback)
# ---------------------------------------------------------------------------


@st.cache_resource(show_spinner="Loading LLM into memory â€” this may take a momentâ€¦")
def load_agents() -> tuple[dict, str, bool]:
    """Loads the LLM and instantiates all agents.

    Returns:
        Tuple of (agents dict, status message, llm_available flag).
        Falls back to mock mode (llm=None) when the GGUF is not cached locally.
    """
    cfg = _load_config()
    from core.agents import create_llm
    from core.agents.therapist import TherapistAgent
    from core.agents.client import ClientAgent
    from core.agents.diagnostician import DiagnosticianAgent
    from core.agents.auditor import EvidenceAuditorAgent
    from core.agents.prompts import (
        get_therapist_prompt,
        get_client_prompt,
        get_diagnostician_prompt,
        get_auditor_prompt,
    )
    from core.retrieval import init_rag_pipeline

    llm = None
    llm_available = False

    try:
        from huggingface_hub import hf_hub_download

        model_path = hf_hub_download(
            repo_id=cfg["llm"]["model_name"],
            filename=cfg["llm"]["model_file"],
            local_files_only=True,  # only use cached copy; never trigger a download here
        )
        llm = create_llm(
            model_path,
            n_ctx=cfg["llm"]["n_ctx"],
            n_gpu_layers=cfg["llm"]["n_gpu_layers"],
            chat_format=cfg["llm"]["chat_format"],
        )
        llm_available = True
    except Exception:
        pass  # mock mode â€” llm stays None

    # Agents are initialised with a neutral language; the system prompt is
    # re-selected per-call via get_<agent>_prompt(state["language"]).
    lang = "English"
    agents = {
        "therapist": TherapistAgent(
            llm,
            system_prompt=get_therapist_prompt(lang),
            temperature=cfg["agents"]["therapist"]["temperature"],
            max_tokens=cfg["agents"]["therapist"]["max_tokens"],
        ),
        "client": ClientAgent(
            llm,
            system_prompt=get_client_prompt(lang),
            temperature=cfg["agents"]["client"]["temperature"],
            max_tokens=cfg["agents"]["client"]["max_tokens"],
        ),
        "diagnostician": DiagnosticianAgent(
            llm,
            system_prompt=get_diagnostician_prompt(lang),
            temperature=cfg["agents"]["diagnostician"]["temperature"],
            max_tokens=cfg["agents"]["diagnostician"]["max_tokens"],
        ),
        "auditor": EvidenceAuditorAgent(
            llm,
            system_prompt=get_auditor_prompt(lang),
            temperature=cfg["agents"]["auditor"]["temperature"],
            max_tokens=cfg["agents"]["auditor"]["max_tokens"],
        ),
    }

    # Initialise RAG pipeline (silent no-op if index has not been built)
    init_rag_pipeline()

    model_file = cfg["llm"]["model_file"]
    status = model_file if llm_available else "mock"
    return agents, status, llm_available


# ---------------------------------------------------------------------------
# Session state helpers
# ---------------------------------------------------------------------------


def _init_app_state() -> None:
    """Initialises Streamlit session_state keys on first page load."""
    if "agents_loaded" not in st.session_state:
        agents, status, llm_available = load_agents()
        AGENTS.update(agents)
        st.session_state.agents_loaded = True
        st.session_state.llm_status = status
        st.session_state.llm_available = llm_available

    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())

    if "graph" not in st.session_state:
        from langgraph.checkpoint.memory import MemorySaver

        memory = MemorySaver()
        st.session_state.graph = build_graph(checkpointer=memory)
        st.session_state.config = {"configurable": {"thread_id": st.session_state.session_id}}

    st.session_state.setdefault("patient_mode", "Auto (Simulated Profile)")
    st.session_state.setdefault("session_language", "English")
    st.session_state.setdefault("started", False)


def _build_initial_state(language: str, interactive: bool, cfg: dict) -> dict:
    """Builds a complete SessionState dict for a fresh session."""
    return {
        "session_id": str(uuid.uuid4()),
        "client_profile": _default_profile(language),
        "transcript": [],
        "messages": [],
        "domains_covered": [],
        "domains_pending": [],  # init_session will shuffle and populate
        "coverage_complete": False,
        "retrieved_chunks": [],
        "query_history": [],
        "hypotheses": [],
        "audit_report": None,
        "risk_detected": False,
        "risk_type": None,
        "current_step": "",
        "turn_count": 0,
        "max_turns": cfg["session"]["max_turns"],
        "finalized": False,
        "interactive_mode": interactive,
        "language": language,
    }


def _default_profile(language: str) -> dict:
    """Returns a minimal synthetic profile used when no JSON profile is loaded."""
    if language == "EspaÃ±ol":
        return {
            "profile_id": "demo_es",
            "demographics": {"name": "Demo", "age": 32, "gender": "no especificado"},
            "presenting_complaints": ["ansiedad", "dificultades para dormir", "irritabilidad"],
            "history": "EstrÃ©s laboral durante los Ãºltimos 4 meses. Sin historial psiquiÃ¡trico previo.",
            "language": "EspaÃ±ol",
        }
    return {
        "profile_id": "demo_en",
        "demographics": {"name": "Demo", "age": 32, "gender": "unspecified"},
        "presenting_complaints": ["anxiety", "sleep difficulties", "irritability"],
        "history": "Work-related stress over the past 4 months. No prior psychiatric history.",
        "language": "English",
    }


# ---------------------------------------------------------------------------
# Graph execution helpers
# ---------------------------------------------------------------------------


def _run_until_interrupt(initial_state: dict | None = None) -> None:
    """Streams the graph forward until it finishes or hits a human_input interrupt."""
    graph = st.session_state.graph
    config = st.session_state.config
    for _ in graph.stream(initial_state, config, stream_mode="values"):
        pass


def _snapshot() -> tuple[dict, bool]:
    """Returns (current_state_values, is_waiting_for_human_input)."""
    snap = st.session_state.graph.get_state(st.session_state.config)
    values = snap.values if snap.values else {}
    waiting = bool(snap.next) and snap.next[0] == "human_input"
    return values, waiting


# ---------------------------------------------------------------------------
# UI rendering
# ---------------------------------------------------------------------------


def _render_sidebar(cfg: dict) -> None:
    with st.sidebar:
        st.header(t("sidebar_header"))

        # LLM / model status
        if st.session_state.get("llm_available"):
            st.success(f"{t('llm_loaded')}: `{st.session_state.llm_status}`")
        else:
            st.warning(t("llm_mock"))

        st.divider()

        # Language selector
        lang_options = ["English", "EspaÃ±ol"]
        current_lang = st.session_state.session_language
        language = st.radio(
            t("language_label"),
            lang_options,
            index=lang_options.index(current_lang),
            disabled=st.session_state.started,
        )
        if language != current_lang:
            st.session_state.session_language = language
            st.rerun()

        # Mode selector â€” labels translate with the current language
        mode_auto = t("mode_auto")
        mode_interactive = t("mode_interactive")
        mode_options = [mode_auto, mode_interactive]
        current_mode = st.session_state.patient_mode
        # Normalise stored value to current-language label
        if current_mode not in mode_options:
            current_mode = mode_auto
        mode = st.radio(
            t("mode_label"),
            mode_options,
            index=mode_options.index(current_mode),
            disabled=st.session_state.started,
        )
        if mode != st.session_state.patient_mode:
            st.session_state.patient_mode = mode

        st.divider()
        max_turns = cfg["session"]["max_turns"]
        n_domains = len(cfg["session"]["domains"])
        st.caption(f"{max_turns} {t('max_turns_caption')} Â· {n_domains} {t('domains_caption')}")
        st.divider()

        if st.button(t("new_session_btn"), use_container_width=True):
            st.session_state.clear()
            st.rerun()


def _render_transcript(transcript: list[dict]) -> None:
    for msg in transcript:
        if msg["role"] == "therapist":
            with st.chat_message("assistant", avatar="ðŸ§‘â€âš•ï¸"):
                st.markdown(msg["content"])
                if domain := msg.get("domain"):
                    st.caption(f"{t('domain_caption')}: `{domain}`")
        elif msg["role"] == "client":
            with st.chat_message("user", avatar="ðŸ‘¤"):
                st.markdown(msg["content"])


def _confidence_icon(confidence: str) -> str:
    return {"HIGH": "ðŸŸ¢", "ALTA": "ðŸŸ¢", "MEDIUM": "ðŸŸ¡", "MEDIA": "ðŸŸ¡"}.get(confidence, "ðŸ”´")


def _render_results(state: dict) -> None:
    if state.get("risk_detected"):
        from core.safety.risk_gate import RiskGate

        st.error(t("risk_terminated"))
        st.markdown(RiskGate().get_safe_response(state.get("risk_type", "")))
        return

    st.success(t("session_complete"))

    col1, col2 = st.columns(2)
    with col1:
        st.subheader(t("hypotheses_header"))
        hypotheses = state.get("hypotheses", [])
        if hypotheses:
            for h in hypotheses:
                confidence = h.get("confidence", "")
                icon = _confidence_icon(confidence)
                label = h.get("label", "?")
                code = h.get("code", "?")
                with st.expander(f"{icon} {label}  â€”  `{code}`"):
                    st.markdown(f"**{t('confidence_label')}:** {confidence}")
                    if h.get("evidence_for"):
                        st.markdown(f"**{t('evidence_for_label')}:**")
                        for e in h["evidence_for"]:
                            st.markdown(f"- {e}")
                    if h.get("evidence_against"):
                        st.markdown(f"**{t('evidence_against_label')}:**")
                        for e in h["evidence_against"]:
                            st.markdown(f"- {e}")
        else:
            st.info(t("no_hypotheses"))

    with col2:
        st.subheader(t("audit_header"))
        audit = state.get("audit_report") or {}
        score = audit.get("traceability_score", 1.0)
        st.metric(t("traceability_metric"), f"{score:.0%}")
        issues = audit.get("issues", [])
        if issues:
            st.warning(t("issues_warning", n=len(issues)))
            for issue in issues:
                st.caption(f"â€¢ [{issue.get('hypothesis', '?')}] {issue.get('claim', '')}")
        else:
            st.success(t("no_issues"))
        if commentary := audit.get("llm_commentary"):
            st.markdown(f"**{t('auditor_comment')}:**")
            st.markdown(f"> {commentary}")

    st.subheader(t("full_transcript"))
    with st.expander(t("view_transcript")):
        st.json(state.get("transcript", []))


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------


def main() -> None:
    cfg = _load_config()

    st.set_page_config(
        page_title=t("page_title"),
        page_icon="ðŸ§ ",
        layout="wide",
    )

    _init_app_state()
    _render_sidebar(cfg)

    st.title(f"ðŸ§  {t('page_title')}")
    st.caption(t("page_subtitle"))
    st.warning(t("disclaimer"))
    st.divider()

    current_state, waiting_for_human = _snapshot()
    transcript = current_state.get("transcript", [])
    _render_transcript(transcript)

    # --- Session finalised ---
    if current_state.get("finalized"):
        _render_results(current_state)
        return

    # --- Session not yet started ---
    if not st.session_state.started:
        lang = st.session_state.session_language
        mode = st.session_state.patient_mode
        is_interactive = t("mode_interactive") in mode

        btn_label = t("btn_start_interactive") if is_interactive else t("btn_start_auto")
        if st.button(btn_label, type="primary"):
            st.session_state.started = True
            initial_state = _build_initial_state(lang, is_interactive, cfg)
            with st.spinner(t("spinner_init")):
                _run_until_interrupt(initial_state)
            st.rerun()
        return

    # --- Interactive mode: waiting for human input ---
    if waiting_for_human:
        user_input = st.chat_input(t("chat_placeholder"))
        if user_input:
            updated_transcript = list(current_state.get("transcript", []))
            updated_transcript.append(
                {
                    "role": "client",
                    "content": user_input,
                    "turn_id": len(updated_transcript),
                }
            )
            st.session_state.graph.update_state(
                st.session_state.config,
                {"transcript": updated_transcript, "current_step": "human_input"},
                as_node="human_input",
            )
            with st.spinner(t("spinner_thinking")):
                _run_until_interrupt()
            st.rerun()
        return

    # --- Auto mode: keep stepping until done ---
    if st.session_state.started and not current_state.get("finalized"):
        turn = current_state.get("turn_count", 0)
        max_turns = current_state.get("max_turns", cfg["session"]["max_turns"])
        pending = len(current_state.get("domains_pending", []))
        covered = len(current_state.get("domains_covered", []))
        st.info(t("progress_info", turn=turn, max=max_turns, pending=pending, covered=covered))

        with st.spinner(t("spinner_simulating")):
            _run_until_interrupt()
            time.sleep(0.4)
        st.rerun()


if __name__ == "__main__":
    main()
